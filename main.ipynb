{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TNT hand gestures study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flow of program\n",
    "DELETE BEFORE SUBMISSION\n",
    "\n",
    "-   code to import data from 3 csv files held in the data folder\n",
    "-   data is then combined in a dictionary and then converted to a pandas dataframe\n",
    "-   the data is then cleaned and the columns are renamed\n",
    "-   the data is then split into a training and testing set\n",
    "-   a linear regression model is then fitted to the training data\n",
    "-   the model is then used to predict the test data\n",
    "-   the mean squared error and r2 score are then calculated\n",
    "-   the results are then printed to the console\n",
    "\n",
    "\n",
    "---\n",
    "# Flow of the Program\n",
    "\n",
    "The program is structured into several sequential phases that collectively process, analyze, and model gesture data acquired from the Phyphox app. Here is a breakdown of each step in the flow:\n",
    "\n",
    "## Data Acquisition\n",
    "1. **Data Import**: The data is imported for processing, collected via the Phyphox app.\n",
    "\n",
    "## Feature Extraction\n",
    "2. **Classifier Selection**: A decision is made on the method to extract features from the data. Options include:\n",
    "   - **DBSCAN**: For density-based clustering, which involves:\n",
    "     - Running permutations of `eps` and `min_samples` values.\n",
    "     - Generating heatmaps to select the optimal parameters.\n",
    "     - Executing the final DBSCAN clustering.\n",
    "   - **KNN**: Applying the K-Nearest Neighbors algorithm.\n",
    "   - **Linear Regression**: Using linear regression models to predict or classify data.\n",
    "   - **Threshold Classification**: Implementing threshold-based classification to separate data into distinct classes.\n",
    "3. **Label Data**: Post-classification, gestures are identified on the timeline.\n",
    "4. **Correction**: Applying methods like diffusion and standard deviation to adjust and correct labels.\n",
    "\n",
    "## Prediction Model\n",
    "5. **Sliding Time Window Setup**:\n",
    "   - Selecting the appropriate step size and window size for sliding time windows.\n",
    "   - Creating dataframes from these windows, marking them as gestures or non-gestures.\n",
    "6. **Data Splitting**: Dividing data into training, testing, and evaluation sets.\n",
    "7. **Model Prediction**: Employing prediction models (such as KNN) to classify the gestures.\n",
    "8. **Evaluation**: Assessing the accuracy and effectiveness of the prediction models.\n",
    "\n",
    "## Presentation of Results\n",
    "9. **Results**: Presenting the findings and outcomes from the analyses and predictions.\n",
    "\n",
    "This structured approach allows for systematic data handling, from acquisition through to the presentation of results, ensuring clarity and efficiency in processing and analyzing gesture data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "#[DELETE BEFORE SUBMISSION]\n",
    "\n",
    "@startuml\n",
    "skinparam monochrome true\n",
    "\n",
    "start\n",
    "\n",
    ":Data Acquisition;\n",
    ":Import Data;\n",
    "note right: Data is collected using the Phyphox app.\n",
    "\n",
    ":Feature Extraction;\n",
    "if (Choose Classifier) then (DBSCAN)\n",
    "  :Run permutations of eps and min_samples values for DBSCAN;\n",
    "  :Run heatmaps to choose the best eps and min_samples;\n",
    "  :Run DBSCAN for final clustering;\n",
    "elseif (KNN) then (KNN)\n",
    "  :Run KNN Classifier;\n",
    "elseif (Linear Regression) then (Linear Regression)\n",
    "  :Run Linear Regression Model;\n",
    "else (Threshold Classification)\n",
    "  :Apply Threshold Classification;\n",
    "endif\n",
    ":Label data by identifying gestures on the timeline;\n",
    ":Apply diffusion and standard deviation to correct labels;\n",
    "\n",
    ":Prediction Model;\n",
    ":Choose right step size and window size for sliding time window;\n",
    ":Make dataframes from sliding windows;\n",
    "note right: Mark dataframes as gestures or non-gestures.\n",
    ":Split train, test, and evaluation data;\n",
    ":Perform prediction using models (KNN, etc.);\n",
    ":Evaluate accuracies of prediction models;\n",
    "\n",
    ":Present Results;\n",
    "\n",
    "stop\n",
    "\n",
    "@enduml\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Installation\n",
    "Before starting our analysis, we need to install several Python libraries that are essential for data manipulation, analysis, and machine learning:\n",
    "\n",
    "- **pandas**: For data handling and manipulation.\n",
    "- **numpy**: For numerical operations.\n",
    "- **matplotlib**: For creating static, animated, and interactive visualizations.\n",
    "- **seaborn**: For high-level interface for drawing attractive and informative statistical graphics.\n",
    "- **scikit-learn**: For implementing machine learning algorithms.\n",
    "\n",
    "These libraries provide the necessary tools to process and analyze the data effectively, supporting our efforts in gesture recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Section\n",
    "\n",
    "\n",
    "## Data Collection Declaration\n",
    "- This project is part of a Data Science and Machine Learning class and was carried out by student developers at the University of Nottingham, adhering to all legal and ethical guidelines.\n",
    "\n",
    "## Legal Aspects\n",
    "- The data collection complied with all applicable laws and university policies. Personal data has been anonymized to protect individual privacy. Note that all use of this data must adhere to relevant data protection and privacy laws, and unauthorized use is prohibited.\n",
    "\n",
    "## Data Information\n",
    "- The dataset is in CSV format with specified columns. Further details about column names and types should be added here.\n",
    "\n",
    "# Importing Libraries\n",
    "\n",
    "The following libraries are imported to facilitate data manipulation, analysis, and machine learning:\n",
    "- **Pandas**: For data handling and manipulation.\n",
    "- **NumPy**: For numerical operations.\n",
    "- **os**: For interacting with the operating system.\n",
    "- **Matplotlib**: For creating visualizations.\n",
    "- **Seaborn**: For enhanced statistical visualizations.\n",
    "- **Scikit-learn**: Includes tools for machine learning, such as model training, regression, classification, and clustering algorithms.\n",
    "- **math**: Provides access to mathematical functions.\n",
    "\n",
    "These libraries form the backbone of our analysis, supporting various data processing, analysis, and machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#-----------------Information-----------------#\n",
    "\n",
    "'''\n",
    "    Title: [Pick up a tile for the paper]\n",
    "    Data Collection Declaration:\n",
    "\n",
    "    This project is being developed for a Data Science and Machine Learning class.\n",
    "    The data used in this project was collected by the student developers at the University of Nottingham. \n",
    "\n",
    "    Legal Aspects:\n",
    "\n",
    "    The data collection process complied with all applicable laws and university policies. \n",
    "    Any personal data that was collected has been anonymized to protect the privacy of the individuals involved. \n",
    "\n",
    "    Please note that the use of this data must comply with all relevant data protection and privacy laws. \n",
    "    Unauthorized use, disclosure, or duplication of this data is strictly prohibited.\n",
    "'''\n",
    "'''\n",
    "    Data Information:\n",
    "\n",
    "    Data within the dataset being examined is of the format of a csv file with the following columns:\n",
    "    Column Names and Types:\n",
    "    \n",
    "'''\n",
    "\n",
    "#-----------------Information-----------------#\n",
    "\n",
    "#-----------------Importing Libraries-----------------#\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "# import k-nearest neighbors\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# importing random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# import SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# import 5 fold cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# import grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "# import min max scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# import DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "#-----------------Importing Libraries-----------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "#-----------------Flags-----------------#\n",
    "\n",
    "SYS_MSG = True # Flag to toggle control over printing system messages to console \n",
    "PLOT_ALL = False # Flag to toggle control over plotting all graphs\n",
    "PLOT = True # Flag to toggle control over plotting graphs\n",
    "FOLLOW_GESTURE = ['circles']\n",
    "FOLLOW_KEY = ['data_anakha_circles_Anticlockwise flat'] # list of Key to follow for plotting graphs. Key is the name of the gesture to be followed, named as per the data file name.\n",
    "# FOLLOW_GESTURE = 'come here'\n",
    "# FOLLOW_KEY = 'data_anakha_come here_Come here horizontal' # Key to follow for plotting graphs. Key is the name of the gesture to be followed, named as per the data file name.\n",
    "# FOLLOW_GESTURE = 'go away'\n",
    "# FOLLOW_KEY = 'data_anakha_go away_Go away horizontal' # Key to follow for plotting graphs. Key is the name of the gesture to be followed, named as per the data file name.\n",
    "# FOLLOW_GESTURE = 'wave'\n",
    "# FOLLOW_KEY = 'data_anakha_wave_Wave flat left focused' # Key to follow for plotting graphs. Key is the name of the gesture to be followed, named as per the data file name.\n",
    "\n",
    "#-----------------Flags-----------------#\n",
    "\n",
    "#-----------------Selecting labelling Classifiers and their parameters-----------------#\n",
    "\n",
    "\n",
    "Labeling_Classifiers = ['DBSCAN'] # List of labelling classifiers to be used. Select for DBSCAN\n",
    "# Labeling_Classifiers = ['KNN'] # List of labelling classifiers to be used. Select for KNN\n",
    "\n",
    "\n",
    "# DBSCAN\n",
    "eps = [0.1, 0.2, 0.3, 0.4, 0.5] # List of eps values to be used for DBSCAN\n",
    "min_samples = [20, 30, 50, 100, 200] # List of min_samples values to be used for DBSCAN\n",
    "aimed_unique_clusters = 4 # Number of unique clusters aimed to be obtained from DBSCAN\n",
    "interpolation_limit = 30 # Limit of interpolation to be used for DBSCAN\n",
    "\n",
    "\n",
    "# K-means\n",
    "\n",
    "#-----------------Selecting labelling Classifiers and their parameters-----------------#\n",
    "\n",
    "#-----------------Sliding Time Window-----------------#\n",
    "\n",
    "# Sliding Time Window Parameters\n",
    "stw = 0 # Sliding Time Window\n",
    "stws = 0 # Sliding Time Window step\n",
    "\n",
    "#-----------------Sliding Time Window-----------------#\n",
    "\n",
    "\n",
    "#-----------------Basic Functions-----------------#\n",
    "\n",
    "# Function to print system messages\n",
    "def print_sys_msg(msg):\n",
    "    if SYS_MSG:\n",
    "        print('-'*10+'System control message'+'-'*10+'\\t\\t'+msg)\n",
    "\n",
    "# Function to print normal messages\n",
    "def print_msg(msg):\n",
    "    print('-'*10+'Message from program'+'-'*10+'\\t'+msg)\n",
    "\n",
    "#-----------------Basic Functions-----------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preperation Phase  \n",
    "We create base classes to manage our data, visualisation, analysis, and prediction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataHandler Class\n",
    "\n",
    "The `DataHandler` class is designed for comprehensive data management, encompassing data import, manipulation, and preprocessing functionalities.\n",
    "\n",
    "## Class Overview\n",
    "- **Purpose**: Manages data through various stages from import to preprocessing, ensuring data is ready for analysis.\n",
    "- **Managed by**: Samarth\n",
    "- **Created on**: 03/02/2024\n",
    "- **Modified on**: 03/02/2024\n",
    "- **Contact**: psxs2@nottingham.ac.uk\n",
    "\n",
    "## Key Functionalities\n",
    "- `import_data(files)`: Imports and combines multiple CSV files into a single DataFrame. Example usage: `data.import_data(['data/member1.csv', 'data/member2.csv'])`\n",
    "- `import_data_system(directories)`: Imports data from specified directories within the system, catering to structured data storage.\n",
    "- `data_shape()`, `data_head()`, `data_info()`, `data_describe()`: Provides basic data explorations like shape, head, info, and descriptive statistics of the DataFrame.\n",
    "- `data_null()`, `data_corr()`: Offers methods to check for null values and compute correlation matrices.\n",
    "- `drop_duplicates()`, `drop_null()`, `drop_negative_time()`: Functions to clean the DataFrame by removing duplicates, null values, and negative time entries.\n",
    "- `min_max_normalization()`, `standardization()`: Methods for data normalization and standardization to prepare data for machine learning algorithms.\n",
    "- `store_data_with_name(file_name)`, `store_data_with_current_date_time()`: Functions to store the processed data into CSV files, optionally using the current date and time as part of the file name.\n",
    "\n",
    "Each function is crafted to simplify the data handling process, ensuring that data is clean, well-organized, and ready for subsequent analysis steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#-----------------DataHandler Class-----------------#\n",
    "'''\n",
    "Class DataHandler\n",
    "    purpose: import and manage data (data manipulation, data wrangling, and data preprocessing)\n",
    "\n",
    "    initialization example:\n",
    "        data = DataHandler()\n",
    "    \n",
    "    functions:\n",
    "    - import_data: import data from csv files and combine them into a single dataframe\n",
    "        dependencies used: pandas\n",
    "        function call example: data.import_data(['data/member1.csv', 'data/member2.csv', 'data/member3.csv'])\n",
    "        input: list of file names\n",
    "        output: modifies internal DataFrame 'df' by combining data from specified files\n",
    "    \n",
    "    - import_data_system: import data from system directories\n",
    "        dependencies used: pandas, os\n",
    "        function call example: data.import_data_system(['data/member1/type_of_gesture/', 'data/member2/type_of_gesture/', 'data/member3/type_of_gesture/'])\n",
    "        input: list of directory paths\n",
    "        output: modifies internal dictionary 'data' by adding dataframes loaded from specified directories\n",
    "        \n",
    "    - data_shape: print the shape of the dataframe\n",
    "        dependencies used: pandas\n",
    "        function call example: data.data_shape()\n",
    "        input: none\n",
    "        output: prints the shape of DataFrame 'df'\n",
    "\n",
    "    - data_head: print the first 5 rows of the dataframe\n",
    "        dependencies used: pandas\n",
    "        function call example: data.data_head()\n",
    "        input: none\n",
    "        output: prints the first five rows of DataFrame 'df'\n",
    "\n",
    "    - data_info: print the information of the dataframe\n",
    "        dependencies used: pandas\n",
    "        function call example: data.data_info()\n",
    "        input: none\n",
    "        output: prints the information of DataFrame 'df'\n",
    "\n",
    "    - data_describe: print the description of the dataframe\n",
    "        dependencies used: pandas\n",
    "        function call example: data.data_describe()\n",
    "        input: none\n",
    "        output: prints statistical description of DataFrame 'df'\n",
    "\n",
    "    - data_null: print the null values in the dataframe\n",
    "        dependencies used: pandas\n",
    "        function call example: data.data_null()\n",
    "        input: none\n",
    "        output: prints the count of null values in each column of DataFrame 'df'\n",
    "    \n",
    "    - data_corr: print the correlation matrix of the dataframe\n",
    "        dependencies used: pandas\n",
    "        function call example: data.data_corr()\n",
    "        input: none\n",
    "        output: prints the correlation matrix of DataFrame 'df'\n",
    "\n",
    "    - drop_duplicates: drop duplicates from the dataframe        \n",
    "        dependencies used: pandas\n",
    "        function call example: data.drop_duplicates()\n",
    "        input: none\n",
    "        output: removes duplicate rows in DataFrame 'df'\n",
    "\n",
    "    - drop_null: drop null values from the dataframe     \n",
    "        dependencies used: pandas\n",
    "        function call example: data.drop_null()\n",
    "        input: none\n",
    "        output: removes rows with null values in DataFrame 'df'\n",
    "    \n",
    "    - drop_negative_time: drop rows with negative time values\n",
    "        dependencies used: pandas\n",
    "        function call example: data.drop_negative_time()\n",
    "        input: none\n",
    "        output: removes rows where 'Time (s)' is negative in DataFrame 'df'\n",
    "\n",
    "    - drop_missing: drop rows with missing values\n",
    "        dependencies used: pandas\n",
    "        function call example: data.drop_missing()\n",
    "        input: none\n",
    "        output: removes rows with missing values in DataFrame 'df'\n",
    "\n",
    "    - fill_missing: fill missing values with mean of the column\n",
    "        dependencies used: pandas\n",
    "        function call example: data.fill_missing()\n",
    "        input: none\n",
    "        output: fills missing values with mean of the column in DataFrame 'df'\n",
    "\n",
    "    - fill_missing_median: fill missing values with median of the column\n",
    "        dependencies used: pandas\n",
    "        function call example: data.fill_missing_median()\n",
    "        input: none\n",
    "        output: fills missing values with median of the column in DataFrame 'df'\n",
    "\n",
    "    - fill_missing_mode: fill missing values with mode of the column\n",
    "        dependencies used: pandas\n",
    "        function call example: data.fill_missing_mode()\n",
    "        input: none\n",
    "        output: fills missing values with mode of the column in DataFrame 'df'\n",
    "\n",
    "    - min_max_normalization: normalize data using min-max normalization\n",
    "        dependencies used: pandas\n",
    "        function call example: data.min_max_normalization()\n",
    "        input: none\n",
    "        output: normalizes data using min-max normalization in DataFrame 'df'\n",
    "\n",
    "    - standardization: standardize data using standardization\n",
    "        dependencies used: pandas\n",
    "        function call example: data.standardization()\n",
    "        input: none\n",
    "        output: standardizes data using standardization in DataFrame 'df'\n",
    "\n",
    "    - add_to_data: add data to the dataframe\n",
    "        dependencies used: pandas\n",
    "        function call example: data.add_to_data(data)\n",
    "        input: data (DataFrame)\n",
    "        output: adds data to DataFrame 'df'\n",
    "\n",
    "    - store_data_with_name: store data to a csv file with a specified name  \n",
    "        dependencies used: pandas\n",
    "        function call example: data.store_data_with_name('data.csv')\n",
    "        input: file_name (string)\n",
    "        output: stores DataFrame 'df' to a csv file with the specified name\n",
    "\n",
    "    - store_data_with_current_date_time: store data to a csv file with current date and time\n",
    "        dependencies used: pandas\n",
    "        function call example: data.store_data_with_current_date_time()\n",
    "        input: none\n",
    "        output: stores DataFrame 'df' to a csv file with the current date and time\n",
    "\n",
    "    - store_data_with_index: store data to a csv file with an index\n",
    "        dependencies used: pandas\n",
    "        function call example: data.store_data_with_index()\n",
    "        input: none\n",
    "        output: stores DataFrame 'df' to a csv file with an index\n",
    "        \n",
    "- Managed by: Samarth\n",
    "- Created on: 03/02/2024\n",
    "- Modified on: 03/02/2024\n",
    "- Contact: psxs2@nottingham.ac.uk\n",
    "'''\n",
    "class DataHandler:\n",
    "    number_of_files = 0\n",
    "    data = None\n",
    "    df = None\n",
    "\n",
    "    def __init__(self):\n",
    "        print_sys_msg('DataHandler:__init__: DataHandler object created')\n",
    "    #-----------------Data Import Functions-----------------#\n",
    "    \n",
    "    # declaration example - data = DataHandler.import_data(['data/member1.csv', 'data/member2.csv', 'data/member3.csv'])\n",
    "    def import_data(self, files):\n",
    "            print_sys_msg('DataHandler:import_data: importing data from 3 csv files and combining them into a single dataframe')\n",
    "            number_of_files = len(files)\n",
    "            for i in range(number_of_files):\n",
    "                if i == 0:\n",
    "                    self.df = pd.read_csv(files[i])\n",
    "                    self.data = {'data_'+str(i+1): pd.read_csv(files[i])}\n",
    "                else:\n",
    "                    self.df = pd.concat([self.df, pd.read_csv(files[i])], ignore_index=True)\n",
    "                    self.data['data_'+str(i+1)] = pd.read_csv(files[i])\n",
    "            print_sys_msg('DataHandler:import_data: data imported successfully')\n",
    "\n",
    "    # declaration example - data = DataHandler.import_data_system(['data/member1/type_of_gesture/', 'data/member2/type_of_gesture/', 'data/member3/type_of_gesture/'])\n",
    "    def import_data_system(self, directorys):\n",
    "        print_sys_msg('DataHandler:import_data: importing data from the system')\n",
    "        \n",
    "        # all files are stored in the format data/member1/type_of_gesture/gesture1/Raw Data.csv\n",
    "\n",
    "        # getting the number of members\n",
    "        number_of_members = len(directorys)\n",
    "\n",
    "        print_sys_msg('DataHandler:import_data: number of members: '+str(number_of_members))\n",
    "\n",
    "        for i in range(number_of_members):\n",
    "            # member name from the directory\n",
    "            member = directorys[i].split('/')[1]\n",
    "            # gesture name from the directory\n",
    "            gesture = directorys[i].split('/')[2]\n",
    "            # getting the number of gestures\n",
    "            number_of_gestures = len(os.listdir(directorys[i]))\n",
    "\n",
    "            print_sys_msg('DataHandler:import_data: member: '+member)\n",
    "            print_sys_msg('DataHandler:import_data: gesture name: '+gesture)\n",
    "            print_sys_msg('DataHandler:import_data: number of gestures: '+str(number_of_gestures))\n",
    "            \n",
    "            for j in range(number_of_gestures):\n",
    "                # reading name of each gesture and then grabbing the 'Raw Data.csv' file from it\n",
    "                gesture_name = os.listdir(directorys[i])[j]\n",
    "                \n",
    "                # reading the 'Raw Data.csv' file inside the gesture folder\n",
    "                # if self.data is None:\n",
    "                #     self.data = {'data_'+member+'_'+gesture+'_'+'_'+str(j+1): pd.read_csv(directorys[i]+gesture_name+'/Raw Data.csv')}\n",
    "                if self.data is None:\n",
    "                    self.data = {'data_'+member+'_'+gesture+'_'+gesture_name: pd.read_csv(directorys[i]+gesture_name+'/Raw Data.csv')}                \n",
    "                # else: \n",
    "                #   self.data['data_'+member+'_'+gesture+'_'+str(j+1)] = pd.read_csv(directorys[i]+gesture_name+'/Raw Data.csv')\n",
    "                else:\n",
    "                    self.data['data_'+member+'_'+gesture+'_'+gesture_name] = pd.read_csv(directorys[i]+gesture_name+'/Raw Data.csv')\n",
    "\n",
    "                # # adding gesture_name to the dictionary of data\n",
    "                # self.data['data_'+member+'_'+gesture+'_'+str(j+1)]['Gesture'] = gesture_name\n",
    "        \n",
    "        print_sys_msg('DataHandler:import_data: data imported successfully')\n",
    "        # print the dictionary names \n",
    "        print_sys_msg('DataHandler:import_data: printing the dictionary names')\n",
    "        print_sys_msg(str(self.data.keys()))\n",
    "        \n",
    "\n",
    "        \n",
    "    #-----------------Data Import Functions-----------------#\n",
    "    #-----------------Folder Creation Functions-----------------#\n",
    "\n",
    "    def create_folder(self, folder_name):\n",
    "        # break the folder name into parts and check if each part exists including the last part\n",
    "        # if not create the folder\n",
    "        print_sys_msg('DataHandler:create_folder: creating folder -> '+folder_name)\n",
    "        parts = folder_name.split('/')\n",
    "        worked_through = ''\n",
    "        for i in range(len(parts)):\n",
    "            print_sys_msg('DataHandler:create_folder: checking if folder exists -> '+parts[i])\n",
    "            if not os.path.exists(worked_through + parts[i]):\n",
    "                print_sys_msg('DataHandler:create_folder: folder does not exist -> '+parts[i])\n",
    "                os.makedirs(worked_through + parts[i])\n",
    "                print_sys_msg('DataHandler:create_folder: folder created -> '+parts[i])\n",
    "            worked_through = worked_through + parts[i] + '/'\n",
    "        print_sys_msg('DataHandler:create_folder: folder created successfully')\n",
    "\n",
    "    #-----------------Folder Creation Functions-----------------#               \n",
    "    \n",
    "    #-----------------Basic Data Wrangling Functions-----------------#\n",
    "    \n",
    "    def data_shape(self):\n",
    "        print_sys_msg('DataHandler:data_shape: printing the shape of the dataframe')\n",
    "        print_sys_msg(str(self.df.shape))\n",
    "        \n",
    "    def data_head(self):\n",
    "        print_sys_msg('DataHandler:data_head: printing the first 5 rows of the dataframe')\n",
    "        print_sys_msg(str(self.df.head()))\n",
    "    \n",
    "    def data_info(self):\n",
    "        print_sys_msg('DataHandler:data_info: printing the information of the dataframe')\n",
    "        print_sys_msg(str(self.df.info()))\n",
    "                      \n",
    "    def data_describe(self):\n",
    "        print_sys_msg('DataHandler:data_describe: printing the description of the dataframe')\n",
    "        print_sys_msg(str(self.df.describe()))\n",
    "                      \n",
    "    def data_null(self):\n",
    "        print_sys_msg('DataHandler:data_null: printing the null values in the dataframe')\n",
    "        print_sys_msg(str(self.df.isnull().sum()))\n",
    "                      \n",
    "    def data_corr(self):\n",
    "        print_sys_msg('DataHandler:data_corr: printing the correlation matrix of the dataframe')\n",
    "        print_sys_msg(str(self.df.corr()))\n",
    "    \n",
    "    def data_missing(self):\n",
    "        print_sys_msg('DataHandler:data_missing: printing the missing values in the dataframe')\n",
    "        print_sys_msg(str(self.df.isna().any(axis=1)))\n",
    "    #-----------------Basic Data Wrangling Functions-----------------#\n",
    "        \n",
    "    #-----------------Data Preprocessing Functions-----------------#\n",
    "\n",
    "    # add_to_data: add data to the dataframe\n",
    "    def add_to_data(self, data):\n",
    "        print_sys_msg('DataHandler:add_to_data: adding data to the dataframe')\n",
    "        self.df = pd.concat([self.df, data], ignore_index=True)\n",
    "\n",
    "    def drop_duplicates(self):\n",
    "        print_sys_msg('DataHandler:drop_duplicates: dropping duplicates')\n",
    "        self.df = self.df.drop_duplicates()\n",
    "    \n",
    "    def drop_null(self):\n",
    "        print_sys_msg('DataHandler:drop_null: dropping null values')\n",
    "        self.df = self.df.dropna()\n",
    "\n",
    "\n",
    "    # def drop_outliers(self):\n",
    "    #     print_sys_msg('DataHandler:drop_outliers: dropping outliers'\n",
    "    #     self.df = self.df[(self.df['Linear Acceleration x (m/s^2)'] > -10) & (self.df['Linear Acceleration x (m/s^2)'] < 10)]\n",
    "    #     self.df = self.df[(self.df['Linear Acceleration y (m/s^2)'] > -10) & (self.df['Linear Acceleration y (m/s^2)'] < 10)]\n",
    "    #     self.df = self.df[(self.df['Linear Acceleration z (m/s^2)'] > -10) & (self.df['Linear Acceleration z (m/s^2)'] < 10)]\n",
    "    #     self.df = self.df[(self.df['Absolute acceleration (m/s^2)'] > 0) & (self.df['Absolute acceleration (m/s^2)'] < 10)]\n",
    "\n",
    "    def drop_negative_time(self):\n",
    "        print_sys_msg('DataHandler:drop_negative_time: dropping negative time values')\n",
    "        self.df = self.df[self.df['Time (s)'] > 0]\n",
    "\n",
    "    # missing values handling - drop rows with missing values\n",
    "    def drop_missing(self, threshold=3):\n",
    "        print_sys_msg('DataHandler:drop_missing: dropping missing values')\n",
    "        self.df = self.df.dropna(thresh=threshold).copy()\n",
    "\n",
    "    # missing values handling - fill missing values with mean of the column\n",
    "    def fill_missing(self):\n",
    "        print_sys_msg('DataHandler:fill_missing: filling missing values with mean of the column')\n",
    "        self.df = self.df.fillna(self.df.mean())\n",
    "\n",
    "    # missing values handling - fill missing values with median of the column\n",
    "    def fill_missing_median(self):\n",
    "        print_sys_msg('DataHandler:fill_missing_median: filling missing values with median of the column')\n",
    "        self.df = self.df.fillna(self.df.median())\n",
    "    \n",
    "    # missing values handling - fill missing values with mode of the column\n",
    "    def fill_missing_mode(self):\n",
    "        print_sys_msg('DataHandler:fill_missing_mode: filling missing values with mode of the column')\n",
    "        self.df = self.df.fillna(self.df.mode().iloc[0])\n",
    "    \n",
    "    # missing values handling - fill missing values with bill debth of the column\n",
    "    #-----------------------------------\n",
    "    #-----------------------------------To be written\n",
    "    #-----------------------------------\n",
    "    \n",
    "    # data normalization - min-max normalization\n",
    "    def min_max_normalization(self):\n",
    "        print_sys_msg('DataHandler:min_max_normalization: min-max normalization')\n",
    "        self.df = (self.df - self.df.min()) / (self.df.max() - self.df.min())\n",
    "    \n",
    "    # data normalization - standardization\n",
    "    def standardization(self):\n",
    "        print_sys_msg('DataHandler:standardization: standardization')\n",
    "        self.df = (self.df - self.df.mean()) / self.df.std()\n",
    "    \n",
    "\n",
    "    #-----------------Data Preprocessing Functions-----------------#\n",
    "\n",
    "\n",
    "    #-----------------Data Splitting Functions-----------------#\n",
    "    \n",
    "    #-----------------Data Splitting Functions-----------------#\n",
    "        \n",
    "    #-----------------Storing Data Functions-----------------#\n",
    "\n",
    "    def store_data_with_name(self, file_name):\n",
    "        print_sys_msg('DataHandler:store_data: storing data to a csv file')\n",
    "        self.df.to_csv(file_name, index=False)\n",
    "    \n",
    "    def store_data_with_current_date_time(self):\n",
    "        print_sys_msg('DataHandler:store_data_with_current_date_time: storing data to a csv file with current date and time')\n",
    "        self.df.to_csv('data_'+str(pd.to_datetime('today'))+'.csv', index=False)\n",
    "    \n",
    "    def store_data_with_index(self):\n",
    "        print_sys_msg('DataHandler:store_data_with_index: storing data to a csv file with index')\n",
    "        \n",
    "        # data is stored with index only\n",
    "        # getting the highest index of the data in the data folder and then incrementing it by 1\n",
    "        # storing the data with the new index\n",
    "        #-----------------------------------\n",
    "        #-----------------------------------To be written\n",
    "\n",
    "    #-----------------Storing Data Functions-----------------#\n",
    "\n",
    "#-----------------DataHandler Class-----------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization Class\n",
    "\n",
    "Next we make a basic visualization class that will manage the plotting and core visualization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#-----------------DataVisualization Class-----------------#\n",
    "'''\n",
    "Class DataVisualization\n",
    "\n",
    "    purpose: visualize data (data visualization)\n",
    "    charts included: line, scatter, bar, histogram, box plot, violin plot, bullet, table, sparkline, connected scatter plot, box, pie, doughnut, gauge, waffle\n",
    "    \n",
    "    functions:\n",
    "    -  __init__: initialize the object\n",
    "        dependencies used: none\n",
    "        function call example: data_visualization = DataVisualization()\n",
    "        input: none\n",
    "        output: none\n",
    "\n",
    "    - switch_to_seaborn: switch to seaborn\n",
    "        dependencies used: none\n",
    "        function call example: data_visualization.switch_to_seaborn(True)\n",
    "        input: boolean flag\n",
    "        output: none\n",
    "\n",
    "    - add_index: add index\n",
    "        dependencies used: none\n",
    "        function call example: data_visualization.add_index(index)\n",
    "        input: index\n",
    "        output: none\n",
    "\n",
    "    - add_data: add data\n",
    "        dependencies used: none\n",
    "        function call example: data_visualization.add_data(data)\n",
    "        input: data\n",
    "        output: none\n",
    "\n",
    "    - plot_chart: plot chart\n",
    "        dependencies used: matplotlib, seaborn\n",
    "        function call example: data_visualization.plot_chart('line', 'cyan', 0.8)\n",
    "        input: type, color, thickness\n",
    "        output: none\n",
    "\n",
    "    - set_grid_params: set grid parameters\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.set_grid_params(2, 2, 20, 5, 'Title')\n",
    "        input: rows, cols, figsize_x, figsize_y, title\n",
    "        output: none\n",
    "\n",
    "    - plot_grid_1d: plot grid 1d\n",
    "        dependencies used: matplotlib, seaborn\n",
    "        function call example: data_visualization.plot_grid_1d(0, 'line', 'x', 'y', 'Title', 'cyan', 0.8)\n",
    "        input: count, type, name_x, name_y, plot_title, color, thickness    \n",
    "        output: none\n",
    "\n",
    "    - plot_grid_2d: plot grid 2d\n",
    "        dependencies used: matplotlib, seaborn\n",
    "        function call example: data_visualization.plot_grid_2d(0, 0, 'line', 'x', 'y', 'Title', 'cyan', 0.8)\n",
    "        input: row, col, type, name_x, name_y, plot_title, color, thickness\n",
    "        output: none\n",
    "\n",
    "    - set_x_label: set x label\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.set_x_label('x')\n",
    "        input: label\n",
    "        output: none\n",
    "\n",
    "    - set_y_label: set y label\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.set_y_label('y')\n",
    "        input: label\n",
    "        output: none\n",
    "\n",
    "    - set_x_tick_labels: set x tick labels\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.set_x_tick_labels(labels)\n",
    "        input: labels\n",
    "        output: none\n",
    "\n",
    "    - set_y_tick_labels: set y tick labels\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.set_y_tick_labels(labels)\n",
    "        input: labels\n",
    "        output: none\n",
    "\n",
    "    - figure_size: set figure size\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.figure_size(20, 5)\n",
    "        input: width, height\n",
    "        output: none\n",
    "\n",
    "    - set_title: set title\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.set_title('Title')\n",
    "        input: title\n",
    "        output: none\n",
    "\n",
    "    - clear_plot: clear plot\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.clear_plot()\n",
    "        input: none\n",
    "        output: none\n",
    "\n",
    "    - show_plot: show plot\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.show_plot()\n",
    "        input: none\n",
    "        output: none\n",
    "   \n",
    "- Managed by: \n",
    "- Created on: 03/02/2024\n",
    "- Modified on: 03/02/2024\n",
    "- Contact: @nottingham.ac.uk\n",
    "'''\n",
    "class DataVisualization:\n",
    "    \n",
    "    # boolean flag to determine if the plot is matplotlib or seaborn\n",
    "    is_seaborn = False\n",
    "\n",
    "    def __init__(self, index):\n",
    "        self.add_index(index)\n",
    "        self.data = []\n",
    "\n",
    "    def switch_to_seaborn(self, flag):\n",
    "        self.is_seaborn = flag\n",
    "\n",
    "    def add_index(self, index):\n",
    "        self.index = index.copy()\n",
    "\n",
    "    def add_data(self, data):\n",
    "        self.data = data.copy()\n",
    "\n",
    "    # def plot_chart(self, type, c='cyan', thickness=0.8, legend=None):\n",
    "    def plot_chart(self, type, name_x, name_y, plot_title, c='cyan', thickness=0.8, legend=None):\n",
    "        # charts included: line, scatter, bar, histogram, box plot, violin plot, bullet, table, sparkline, connected scatter plot, box, pie, doughnut, gauge, waffle\n",
    "        if type == 'line':\n",
    "            sns.lineplot(x=self.index, y=self.data, c=c, linewidth=thickness) if self.is_seaborn else plt.plot(self.index, self.data, c=c, linewidth=thickness)\n",
    "        elif type == 'scatter':\n",
    "            sns.scatterplot(x=self.index, y=self.data, c=c) if self.is_seaborn else plt.scatter(self.index, self.data, c=c)\n",
    "        elif type == 'bar':\n",
    "            sns.barplot(x=self.index, y=self.data, c=c) if self.is_seaborn else plt.bar(self.index, self.data, c=c)\n",
    "        elif type == 'histogram':\n",
    "            sns.histplot(self.data, c=c) if self.is_seaborn else plt.hist(self.data, c=c)\n",
    "        elif type == 'box plot':\n",
    "            sns.boxplot(self.data, c=c) if self.is_seaborn else plt.boxplot(self.data, c=c)\n",
    "        elif type == 'violin plot':\n",
    "            sns.violinplot(self.data, c=c) if self.is_seaborn else plt.violinplot(self.data, c=c)\n",
    "        elif type == 'bullet':\n",
    "            sns.bullet(self.data, c=c) if self.is_seaborn else plt.bullet(self.data, c=c)\n",
    "        elif type == 'table':\n",
    "            sns.table(self.data, c=c) if self.is_seaborn else plt.table(self.data, c=c)\n",
    "        elif type == 'sparkline':\n",
    "            sns.sparkline(self.data, c=c) if self.is_seaborn else plt.sparkline(self.data, c=c)\n",
    "        elif type == 'connected scatter plot':\n",
    "            sns.lineplot(x=self.index, y=self.data, sort=False, c=c) if self.is_seaborn else plt.plot(self.index, self.data, c=c)\n",
    "        elif type == 'box':\n",
    "            sns.boxplot(self.data, c=c) if self.is_seaborn else plt.boxplot(self.data, c=c)\n",
    "        elif type == 'pie':\n",
    "            plt.pie(self.data, labels=self.index)\n",
    "        elif type == 'doughnut':\n",
    "            plt.pie(self.data, labels=self.index, wedgeprops=dict(width=0.5))\n",
    "        elif type == 'gauge':\n",
    "            plt.pie(self.data, labels=self.index, wedgeprops=dict(width=0.2))\n",
    "        elif type == 'waffle':\n",
    "            plt.pie(self.data, labels=self.index, wedgeprops=dict(width=0.1))\n",
    "        else:\n",
    "            print('Invalid chart type')\n",
    "        \n",
    "        if legend is not None:\n",
    "            plt.legend(legend, loc='upper right')\n",
    "    \n",
    "    def set_grid_params(self, rows, cols, figsize_x, figsize_y, title):\n",
    "        if rows <= 0 or cols <= 0:\n",
    "            print_sys_msg('DataVisualization:set_grid_params: rows and cols should be greater than 0')\n",
    "            return\n",
    "        self.fig, self.ax = plt.subplots(rows, cols, figsize=(figsize_x, figsize_y))\n",
    "        self.fig.suptitle(title)\n",
    "    \n",
    "    def plot_grid_1d(self, count, type, name_x, name_y, plot_title, c= 'cyan', thickness=0.8, legend=None):\n",
    "        \n",
    "        if count <= 1:\n",
    "            self.plot_chart(type, color, thickness)\n",
    "            return\n",
    "        self.ax[count].set_xlabel(name_x)\n",
    "        self.ax[count].set_ylabel(name_y)\n",
    "        self.ax[count].set_title(plot_title)\n",
    "        if type == 'line':\n",
    "            sns.lineplot(x=self.index, y=self.data, c=c, linewidth=thickness, ax=self.ax[count]) if self.is_seaborn else self.ax[count].plot(self.index, self.data, c=c, linewidth=thickness)\n",
    "        elif type == 'scatter':\n",
    "            sns.scatterplot(x=self.index, y=self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].scatter(self.index, self.data, c=c)\n",
    "        elif type == 'bar':\n",
    "            sns.barplot(x=self.index, y=self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].bar(self.index, self.data, c=c)\n",
    "        elif type == 'histogram':\n",
    "            sns.histplot(self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].hist(self.data, c=c)\n",
    "        elif type == 'box plot':\n",
    "            sns.boxplot(self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].boxplot(self.data, c=c)\n",
    "        elif type == 'violin plot':\n",
    "            sns.violinplot(self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].violinplot(self.data, c=c)\n",
    "        elif type == 'bullet':\n",
    "            sns.bullet(self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].bullet(self.data, c=c)\n",
    "        elif type == 'table':\n",
    "            sns.table(self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].table(self.data, c=c)\n",
    "        elif type == 'sparkline':\n",
    "            sns.sparkline(self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].sparkline(self.data, c=c)\n",
    "        elif type == 'connected scatter plot':\n",
    "            sns.lineplot(x=self.index, y=self.data, sort=False, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].plot(self.index, self.data, c=c)\n",
    "        elif type == 'box':\n",
    "            sns.boxplot(self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].boxplot(self.data, c=c)\n",
    "        elif type == 'pie':\n",
    "            self.ax[count].pie(self.data, labels=self.index)\n",
    "        elif type == 'doughnut':\n",
    "            self.ax[count].pie(self.data, labels=self.index, wedgeprops=dict(width=0.5))\n",
    "        elif type == 'gauge':\n",
    "            self.ax[count].pie(self.data, labels=self.index, wedgeprops=dict(width=0.2))\n",
    "        elif type == 'waffle':\n",
    "            self.ax[count].pie(self.data, labels=self.index, wedgeprops=dict(width=0.1))\n",
    "        else:\n",
    "            print('Invalid chart type')\n",
    "\n",
    "        if legend is not None:\n",
    "            self.ax[count].legend(legend, loc='upper right')\n",
    "\n",
    "    def plot_grid_2d(self, row, col, type, name_x, name_y, plot_title, c='cyan', thickness=0.8, legend=None):\n",
    "        self.ax[row, col].set_xlabel(name_x)\n",
    "        self.ax[row, col].set_ylabel(name_y)\n",
    "        self.ax[row, col].set_title(plot_title)\n",
    "\n",
    "        if type == 'line':\n",
    "            sns.lineplot(x=self.index, y=self.data, c=c, linewidth=thickness, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].plot(self.index, self.data, c=c, linewidth=thickness)\n",
    "        elif type == 'scatter':\n",
    "            sns.scatterplot(x=self.index, y=self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].scatter(self.index, self.data, c=c)\n",
    "        elif type == 'bar':\n",
    "            sns.barplot(x=self.index, y=self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].bar(self.index, self.data, c=c)\n",
    "        elif type == 'histogram':\n",
    "            sns.histplot(self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].hist(self.data, c=c)\n",
    "        elif type == 'box plot':\n",
    "            sns.boxplot(self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].boxplot(self.data, c=c)\n",
    "        elif type == 'violin plot':\n",
    "            sns.violinplot(self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].violinplot(self.data, c=c)\n",
    "        elif type == 'bullet':\n",
    "            sns.bullet(self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].bullet(self.data, c=c)\n",
    "        elif type == 'table':\n",
    "            sns.table(self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].table(self.data, c=c)\n",
    "        elif type == 'sparkline':\n",
    "            sns.sparkline(self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].sparkline(self.data, c=c)\n",
    "        elif type == 'connected scatter plot':\n",
    "            sns.lineplot(x=self.index, y=self.data, sort=False, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].plot(self.index, self.data, c=c)\n",
    "        elif type == 'box':\n",
    "            sns.boxplot(self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].boxplot(self.data, c=c)\n",
    "        elif type == 'pie':\n",
    "            self.ax[row, col].pie(self.data, labels=self.index)\n",
    "        elif type == 'doughnut':\n",
    "            self.ax[row, col].pie(self.data, labels=self.index, wedgeprops=dict(width=0.5))\n",
    "        elif type == 'gauge':\n",
    "            self.ax[row, col].pie(self.data, labels=self.index, wedgeprops=dict(width=0.2))\n",
    "        elif type == 'waffle':\n",
    "            self.ax[row, col].pie(self.data, labels=self.index, wedgeprops=dict(width=0.1))\n",
    "        else:\n",
    "            print('Invalid chart type')\n",
    "\n",
    "        if legend is not None:\n",
    "            self.ax[row, col].legend(legend, loc='upper right')\n",
    "\n",
    "    def set_x_label(self, label):\n",
    "        plt.xlabel(label)\n",
    "\n",
    "    def set_y_label(self, label):\n",
    "        plt.ylabel(label)\n",
    "\n",
    "    def set_x_tick_labels(self, labels):\n",
    "        plt.xticks(self.index, labels)\n",
    "    \n",
    "    def set_y_tick_labels(self, labels):\n",
    "        plt.yticks(self.index, labels)\n",
    "    \n",
    "    def figure_size(self, width = 20, height = 5):\n",
    "        plt.figure(figsize=(width, height))\n",
    "    \n",
    "    def set_title(self, title):\n",
    "        plt.title(title)\n",
    "\n",
    "    def clear_plot(self):\n",
    "        plt.clf()\n",
    "        \n",
    "    def show_plot(self):\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA Acquisition Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "importing 3 seperate member files containing the data from phyphox of each member of the team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_circles = DataHandler()\n",
    "# data_circles.import_data_system(['data/anakha/circles/', 'data/jordan/circles/'])\n",
    "data_circles.import_data_system(['data/anakha/circles/'])\n",
    "data_circles.name='circles'\n",
    "\n",
    "data_come_here = DataHandler()\n",
    "# data_come_here.import_data_system(['data/anakha/come here/', 'data/jordan/come here/'])\n",
    "data_come_here.import_data_system(['data/anakha/come here/'])\n",
    "data_come_here.name='come here'\n",
    "\n",
    "data_go_away = DataHandler()\n",
    "# data_go_away.import_data_system(['data/anakha/go away/', 'data/jordan/go away/'])\n",
    "data_go_away.import_data_system(['data/anakha/go away/'])\n",
    "data_go_away.name='go away'\n",
    "\n",
    "data_wave = DataHandler()\n",
    "# data_wave.import_data_system(['data/anakha/wave/', 'data/jordan/wave/'])\n",
    "data_wave.import_data_system(['data/anakha/wave/'])\n",
    "data_wave.name='wave'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "Visualize_this_cell = False\n",
    "if Visualize_this_cell:\n",
    "    if PLOT_ALL == True or PLOT == True:\n",
    "        if Visualize_this_cell: # set to True to visualize the raw data\n",
    "            # show linear accelaration x, y, z in a plot alongside Absolute acceleration to the right in a grid for circle the keys in the dictionary data.data\n",
    "            for gesture in [data_circles, data_come_here, data_go_away, data_wave]:\n",
    "                # raw data maps will be stored in the folder raw_data_maps/[gesture_name]\n",
    "                gesture.create_folder('raw_data_visualization/'+gesture.name)\n",
    "                for key in gesture.data.keys():\n",
    "                    if PLOT_ALL == True or (gesture.name in FOLLOW_GESTURE and key in FOLLOW_KEY):\n",
    "                        print_sys_msg(key) \n",
    "                        dv = DataVisualization(gesture.data[key]['Time (s)'])\n",
    "                        dv.set_grid_params(2, 2, 20, 15, gesture.name+' -> '+key)\n",
    "                        dv.add_data(gesture.data[key]['Linear Acceleration x (m/s^2)'])\n",
    "                        dv.plot_grid_2d(0, 0, 'line', 'Time (s)', 'Linear Acceleration x (m/s^2)', 'Linear Acceleration x (m/s^2) vs Time (s)', \"red\")\n",
    "                        dv.add_data(gesture.data[key]['Linear Acceleration y (m/s^2)'])\n",
    "                        dv.plot_grid_2d(0, 1, 'line', 'Time (s)', 'Linear Acceleration y (m/s^2)', 'Linear Acceleration y (m/s^2) vs Time (s)', \"green\")\n",
    "                        dv.add_data(gesture.data[key]['Linear Acceleration z (m/s^2)'])\n",
    "                        dv.plot_grid_2d(1, 0, 'line', 'Time (s)', 'Linear Acceleration z (m/s^2)', 'Linear Acceleration z (m/s^2) vs Time (s)', \"blue\")\n",
    "                        dv.add_data(gesture.data[key]['Absolute acceleration (m/s^2)'])\n",
    "                        dv.plot_grid_2d(1, 1, 'line', 'Time (s)', 'Absolute acceleration (m/s^2)', 'Absolute acceleration (m/s^2) vs Time (s)')\n",
    "                        dv.show_plot()\n",
    "                        # storing the raw data maps\n",
    "                        dv.fig.savefig('raw_data_visualization/'+gesture.name+'/'+key+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN for labelling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DBSCANClustering. it will two groups using DBSCAN clustering algorithm for each column in the dataframe except the time, and gesture column. Also it will have a function to visualize it\n",
    "class DBSCANClustering:\n",
    "    def __init__(self, data, eps, min_samples):\n",
    "        self.data = data\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "        self.clusters = [] # 0 -> linear acceleration x, 1 -> linear acceleration y, 2 -> linear acceleration z, 3 -> absolute acceleration\n",
    "        \n",
    "    \n",
    "    def cluster(self):\n",
    "        for column in self.data.columns:\n",
    "            print_sys_msg('DBSCANClustering:cluster: column: '+column)\n",
    "            if column != 'Time (s)' and column != 'Gesture':\n",
    "                print_sys_msg('DBSCANClustering:cluster: clustering started for column: '+column)  \n",
    "                clustering = DBSCAN(eps=self.eps, min_samples=self.min_samples).fit(self.data[column].values.reshape(-1, 1))\n",
    "                self.clusters.append(clustering.labels_)\n",
    "        print_sys_msg('DBSCANClustering:cluster: clustering done')\n",
    "        print_sys_msg('DBSCANClustering:cluster: clusters: '+str(len(self.clusters)))\n",
    "        return self.clusters\n",
    "\n",
    "    def print_unique_clusters_count_and_density(self):\n",
    "        # cycling though acceleration x, y, z, and absolute acceleration\n",
    "        print_msg('unique clusters found in linear acceleration x: '+str(np.unique(self.clusters[0]))+' ----- total clusters: '+str(len(np.unique(self.clusters[0]))))\n",
    "        print_msg('unique clusters found in linear acceleration x has the following cluster density: ')\n",
    "        for cluster in np.unique(self.clusters[0]):\n",
    "            print_msg('cluster: '+str(cluster)+' density: '+str(len(self.clusters[0][self.clusters[0] == cluster])))\n",
    "        print_msg('unique clusters found in linear acceleration y: '+str(np.unique(self.clusters[1]))+' ----- total clusters: '+str(len(np.unique(self.clusters[1]))))\n",
    "        print_msg('unique clusters found in linear acceleration y has the following cluster density: ')\n",
    "        for cluster in np.unique(self.clusters[1]):\n",
    "            print_msg('cluster: '+str(cluster)+' density: '+str(len(self.clusters[1][self.clusters[1] == cluster])))\n",
    "        print_msg('unique clusters found in linear acceleration z: '+str(np.unique(self.clusters[2]))+' ----- total clusters: '+str(len(np.unique(self.clusters[2]))))\n",
    "        print_msg('unique clusters found in linear acceleration z has the follwing cluster density: ')\n",
    "        for cluster in np.unique(self.clusters[2]):\n",
    "            print_msg('cluster: '+str(cluster)+' density: '+str(len(self.clusters[2][self.clusters[2] == cluster])))\n",
    "        print_msg('unique clusters found in absolute acceleration: '+str(np.unique(self.clusters[3]))+' ----- total clusters: '+str(len(np.unique(self.clusters[3]))))\n",
    "        print_msg('unique clusters found in absolute acceleration has the follwing cluster density: ')\n",
    "        for cluster in np.unique(self.clusters[3]):\n",
    "            print_msg('cluster: '+str(cluster)+' density: '+str(len(self.clusters[3][self.clusters[3] == cluster])))\n",
    "    \n",
    "    def visualize_with_DataVisualization(self, include_original_data = False, sns = False):\n",
    "        for i in range(len(self.clusters)):\n",
    "            if self.data.columns[i] != 'Time (s)':\n",
    "                dv = DataVisualization(self.data['Time (s)'])\n",
    "                if sns == True:\n",
    "                    dv.switch_to_seaborn(True)\n",
    "                dv.add_data(self.data.iloc[:, i])\n",
    "                dv.plot_chart('scatter', 'cyan', 0.8)\n",
    "                if include_original_data == True:\n",
    "                    dv.plot_chart('line', 'red', 0.3)\n",
    "                dv.set_x_label('Time (s)')\n",
    "                dv.set_y_label(self.data.columns[i])\n",
    "                dv.set_title(self.data.columns[i])\n",
    "                dv.show_plot()\n",
    "                if sns == True:\n",
    "                    dv.switch_to_seaborn(False)\n",
    "    \n",
    "    def visualize_with_DataVisualization_selected_column(self, column, include_original_data = False, sns = False): # column is the index of the column. 1- linear acceleration x, 2- linear acceleration y, 3- linear acceleration z, 4- absolute acceleration\n",
    "        dv = DataVisualization(self.data['Time (s)'])\n",
    "        if sns == True:\n",
    "            dv.switch_to_seaborn(True)\n",
    "        dv.add_data(self.data.iloc[:, column])\n",
    "        dv.plot_chart('scatter', c=self.clusters[column], thickness=0.8)\n",
    "        if include_original_data == True:\n",
    "            dv.plot_chart('line', c='red', thickness=0.3)\n",
    "        dv.set_x_label('Time (s)')\n",
    "        dv.set_y_label(self.data.columns[column])\n",
    "        dv.set_title(self.data.columns[column])\n",
    "        dv.show_plot()\n",
    "        if sns == True:\n",
    "            dv.switch_to_seaborn(False)\n",
    "        \n",
    "    def visualize_with_DataVisualization_2d(self, topic = 'DBSCAN Clustering', include_original_data = False, sns = False, save=False, file_path_name='DBSCAN Clustering', visualize_in_terminal=False):\n",
    "        # use dv.plot_grid_2d\n",
    "        dv = DataVisualization(self.data['Time (s)'])\n",
    "        if sns == True:\n",
    "            dv.switch_to_seaborn(True)\n",
    "\n",
    "        dv.set_grid_params(2, 2, 20, 10, topic)\n",
    "\n",
    "        dv.add_data(self.data.iloc[:, 1])\n",
    "        dv.plot_grid_2d(0, 0, 'scatter', 'Time (s)', self.data.columns[1], self.data.columns[1], c=self.clusters[0])\n",
    "        if include_original_data == True:\n",
    "            dv.plot_grid_2d(0, 0, 'line' , 'Time (s)', self.data.columns[1], self.data.columns[1], c='red', thickness=0.3)\n",
    "\n",
    "        dv.add_data(self.data.iloc[:, 2])\n",
    "        dv.plot_grid_2d(0, 1, 'scatter', 'Time (s)', self.data.columns[2], self.data.columns[2], c=self.clusters[1])\n",
    "        if include_original_data == True:\n",
    "            dv.plot_grid_2d(0, 1, 'line' , 'Time (s)', self.data.columns[2], self.data.columns[2], c='red', thickness=0.3)\n",
    "\n",
    "        dv.add_data(self.data.iloc[:, 3])\n",
    "        dv.plot_grid_2d(1, 0, 'scatter', 'Time (s)', self.data.columns[3], self.data.columns[3], c=self.clusters[2])\n",
    "        if include_original_data == True:\n",
    "            dv.plot_grid_2d(1, 0, 'line' , 'Time (s)', self.data.columns[3], self.data.columns[3], c='red', thickness=0.3)\n",
    "            \n",
    "        dv.add_data(self.data.iloc[:, 4])\n",
    "        dv.plot_grid_2d(1, 1, 'scatter', 'Time (s)', self.data.columns[4], self.data.columns[4], c=self.clusters[3])\n",
    "        if include_original_data == True:\n",
    "            dv.plot_grid_2d(1, 1, 'line' , 'Time (s)', self.data.columns[4], self.data.columns[4], c='red', thickness=0.3)\n",
    "        \n",
    "        if visualize_in_terminal == True:\n",
    "            dv.show_plot()\n",
    "        if sns == True:\n",
    "            dv.switch_to_seaborn(False)\n",
    "        if save == True:\n",
    "            dv.fig.savefig(file_path_name)\n",
    "\n",
    "    def Visualize_best_esp_min_samples(self, best_values_df, eps, min_samples, include_original_data = False, sns = False, save=False, file_path_name='DBSCAN Clustering'):\n",
    "        # create a heatmap with index as eps and columns as min_samples and values as the number of unique clusters found in linear acceleration x, linear acceleration y, linear acceleration z, and absolute acceleration for each key in best_values_df.\n",
    "        self.best_values_df = best_values_df\n",
    "        dv = DataVisualization(self.best_values_df['eps'])\n",
    "        keys = self.best_values_df['key'].unique()\n",
    "        for key in keys:\n",
    "            # create a heatmap for each key in best_values_df with index as eps and columns as min_samples and values as the number of unique clusters found in linear acceleration x, linear acceleration y, linear acceleration z, and absolute acceleration. make 4 seperate heatmaps for each acceleration in a grid of 2 x 2. use sns.heatmap to create the heatmap\n",
    "            dv.switch_to_seaborn(True)\n",
    "            dv.set_grid_params(2, 2, 20, 15, key)\n",
    "            sns.heatmap(self.best_values_df[self.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_linear_acceleration_x'), ax=dv.ax[0, 0], cmap='coolwarm')\n",
    "            dv.ax[0, 0].set_title('unique_clusters_linear_acceleration_x')\n",
    "            sns.heatmap(self.best_values_df[self.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_linear_acceleration_y'), ax=dv.ax[0, 1], cmap='coolwarm')\n",
    "            dv.ax[0, 1].set_title('unique_clusters_linear_acceleration_y')\n",
    "            sns.heatmap(self.best_values_df[self.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_linear_acceleration_z'), ax=dv.ax[1, 0], cmap='coolwarm')\n",
    "            dv.ax[1, 0].set_title('unique_clusters_linear_acceleration_z')\n",
    "            sns.heatmap(self.best_values_df[self.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_absolute_acceleration'), ax=dv.ax[1, 1], cmap='coolwarm')\n",
    "            dv.ax[1, 1].set_title('unique_clusters_absolute_acceleration')\n",
    "            dv.show_plot()\n",
    "            dv.switch_to_seaborn(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "Visualize_this_cell = False\n",
    "# visualizations for DBSCAN clustering will be stored in the folder DBSCAN_cluster_data/[gesture_\n",
    "if 'DBSCAN' in Labeling_Classifiers:\n",
    "    for gesture in [data_circles, data_come_here, data_go_away, data_wave]:\n",
    "        print_sys_msg(\"starting pre-labelling DBSCAN analysis for gesture: \"+gesture.name)\n",
    "\n",
    "        best_values = []  # List to store the data of the best values of eps and min_samples\n",
    "\n",
    "        for key in gesture.data.keys():\n",
    "            print_sys_msg(key)\n",
    "            # create folder to store the cluster data\n",
    "            folder_name = 'DBSCAN_cluster_data/'+gesture.name\n",
    "            \n",
    "            for e in eps:\n",
    "                for m in min_samples:\n",
    "                    print_msg(key+' -> eps: '+str(e)+' min_samples: '+str(m))\n",
    "\n",
    "                    dbscan = DBSCANClustering(gesture.data[key], e, m)\n",
    "                    dbscan.cluster()\n",
    "                    # print unique clusters found in linear acceleration x, linear acceleration y, linear acceleration z, and absolute acceleration\n",
    "                    dbscan.print_unique_clusters_count_and_density()\n",
    "                    # visualize a particular key in [gesture].data using DBSCAN clustering algorithm with different values of eps and min_samples\n",
    "                    if Visualize_this_cell:\n",
    "                        if PLOT_ALL == True or PLOT == True:\n",
    "                            if PLOT_ALL == True or (gesture.name in FOLLOW_GESTURE and key in FOLLOW_KEY):\n",
    "                                gesture.create_folder(folder_name)\n",
    "                                dbscan.visualize_with_DataVisualization_2d(key+' '+str(e)+' '+str(m), include_original_data=True, sns=True, save=True, file_path_name=folder_name+'/'+key+'_eps_'+str(e)+'_min_samples_'+str(m)+'.png', visualize_in_terminal=True)\n",
    "\n",
    "                    best_values.append({\n",
    "                        'key': key, 'eps': e, 'min_samples': m,\n",
    "                        'unique_clusters_linear_acceleration_x': len(np.unique(dbscan.clusters[0])),\n",
    "                        'unique_clusters_linear_acceleration_y': len(np.unique(dbscan.clusters[1])),\n",
    "                        'unique_clusters_linear_acceleration_z': len(np.unique(dbscan.clusters[2])),\n",
    "                        'unique_clusters_absolute_acceleration': len(np.unique(dbscan.clusters[3])),\n",
    "                        'cluster_density_linear_acceleration_x': [len(dbscan.clusters[0][dbscan.clusters[0] == cluster]) for cluster in np.unique(dbscan.clusters[0])],\n",
    "                        'cluster_density_linear_acceleration_y': [len(dbscan.clusters[1][dbscan.clusters[1] == cluster]) for cluster in np.unique(dbscan.clusters[1])],\n",
    "                        'cluster_density_linear_acceleration_z': [len(dbscan.clusters[2][dbscan.clusters[2] == cluster]) for cluster in np.unique(dbscan.clusters[2])],\n",
    "                        'cluster_density_absolute_acceleration': [len(dbscan.clusters[3][dbscan.clusters[3] == cluster]) for cluster in np.unique(dbscan.clusters[3])]\n",
    "                    })\n",
    "\n",
    "        gesture.best_values_df = pd.DataFrame(best_values)\n",
    "        print_msg('gesture.best_value_df: '+str(gesture.best_values_df))\n",
    "\n",
    "        # print gesture.best_values_df to a xlsx file\n",
    "        gesture.best_values_df.to_excel('best_values_df.xlsx', index=False)\n",
    "        gesture.tested_eps = eps\n",
    "        gesture.tested_min_samples = min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "Visualize_this_cell = True\n",
    "# visualizations for heatmap of eps and min_samples will be stored in the folder DBSCAN_cluster_eps_min_samples_heatmap/[gesture_name]\n",
    "if 'DBSCAN' in Labeling_Classifiers:\n",
    "    for gesture in [data_circles, data_come_here, data_go_away, data_wave]:\n",
    "        keys = gesture.best_values_df['key'].unique()\n",
    "\n",
    "        # -----------------Data Visualization of eps and min_samples-----------------\n",
    "        if Visualize_this_cell:\n",
    "            if PLOT_ALL == True or PLOT == True:\n",
    "                    print_sys_msg('starting data visualization of eps and min_samples for gesture: '+gesture.name)\n",
    "                    # create a heatmap with index as eps and columns as min_samples and values as the number of unique clusters found in linear acceleration x, linear acceleration y, linear acceleration z, and absolute acceleration for each key in best_values_df.\n",
    "                    dv = DataVisualization(gesture.best_values_df['eps'])\n",
    "                    folder_name = 'DBSCAN_cluster_eps_min_samples_heatmap/'+gesture.name\n",
    "                    gesture.create_folder(folder_name)\n",
    "                    for key in keys:\n",
    "                        if PLOT_ALL == True or (gesture.name in FOLLOW_GESTURE and key in FOLLOW_KEY):\n",
    "                            print_sys_msg('visualizing eps and min_samples for key: '+key)\n",
    "                            # create a heatmap for each key in gesture.best_values_df with index as eps and columns as min_samples and values as the number of unique clusters found in linear acceleration x, linear acceleration y, linear acceleration z, and absolute acceleration. make 4 seperate heatmaps for each acceleration in a grid of 2 x 2. use sns.heatmap to create the heatmap\n",
    "                            dv.switch_to_seaborn(True)\n",
    "                            dv.set_grid_params(2, 2, 20, 15, key)\n",
    "                            sns.heatmap(gesture.best_values_df[gesture.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_linear_acceleration_x'), ax=dv.ax[0, 0], cmap='coolwarm')\n",
    "                            dv.ax[0, 0].set_title('unique_clusters_linear_acceleration_x')\n",
    "                            sns.heatmap(gesture.best_values_df[gesture.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_linear_acceleration_y'), ax=dv.ax[0, 1], cmap='coolwarm')\n",
    "                            dv.ax[0, 1].set_title('unique_clusters_linear_acceleration_y')\n",
    "                            sns.heatmap(gesture.best_values_df[gesture.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_linear_acceleration_z'), ax=dv.ax[1, 0], cmap='coolwarm')\n",
    "                            dv.ax[1, 0].set_title('unique_clusters_linear_acceleration_z')\n",
    "                            sns.heatmap(gesture.best_values_df[gesture.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_absolute_acceleration'), ax=dv.ax[1, 1], cmap='coolwarm')\n",
    "                            dv.ax[1, 1].set_title('unique_clusters_absolute_acceleration')\n",
    "                            dv.show_plot()\n",
    "                            dv.switch_to_seaborn(False)\n",
    "\n",
    "                            # save heatmap in folder cluster_eps_min_samples_heatmap.\n",
    "                            dv.fig.savefig(folder_name+'/'+key+'.png')\n",
    "\n",
    "        # -----------------Data Visualization of eps and min_samples-----------------\n",
    "\n",
    "        # -----------------Selecting the best values of eps and min_samples----------------\n",
    "\n",
    "        # testing calculating the best values of eps and min_samples together.\n",
    "        min_samples_and_eps_mean_unique_clusters = []\n",
    "        for e in eps:\n",
    "            for m in min_samples:\n",
    "                mean_unique_clusters = []\n",
    "                for key in keys:\n",
    "                    mean_unique_clusters.append({\n",
    "                        'key': key,\n",
    "                        'mean_unique_clusters_linear_acceleration_x': gesture.best_values_df[(gesture.best_values_df['key'] == key) & (gesture.best_values_df['eps'] == e) & (gesture.best_values_df['min_samples'] == m)]['unique_clusters_linear_acceleration_x'].mean(),\n",
    "                        'mean_unique_clusters_linear_acceleration_y': gesture.best_values_df[(gesture.best_values_df['key'] == key) & (gesture.best_values_df['eps'] == e) & (gesture.best_values_df['min_samples'] == m)]['unique_clusters_linear_acceleration_y'].mean(),\n",
    "                        'mean_unique_clusters_linear_acceleration_z': gesture.best_values_df[(gesture.best_values_df['key'] == key) & (gesture.best_values_df['eps'] == e) & (gesture.best_values_df['min_samples'] == m)]['unique_clusters_linear_acceleration_z'].mean(),\n",
    "                        'mean_unique_clusters_absolute_acceleration': gesture.best_values_df[(gesture.best_values_df['key'] == key) & (gesture.best_values_df['eps'] == e) & (gesture.best_values_df['min_samples'] == m)]['unique_clusters_absolute_acceleration'].mean()\n",
    "                    })\n",
    "                # mean is calculated a second time to get the mean of the mean of the unique clusters for each key.\n",
    "                min_samples_and_eps_mean_unique_clusters.append({\n",
    "                    'eps': e,\n",
    "                    'min_samples': m,\n",
    "                    'mean_unique_clusters_linear_acceleration_x': pd.DataFrame(mean_unique_clusters)['mean_unique_clusters_linear_acceleration_x'].mean(),\n",
    "                    'mean_unique_clusters_linear_acceleration_y': pd.DataFrame(mean_unique_clusters)['mean_unique_clusters_linear_acceleration_y'].mean(),\n",
    "                    'mean_unique_clusters_linear_acceleration_z': pd.DataFrame(mean_unique_clusters)['mean_unique_clusters_linear_acceleration_z'].mean(),\n",
    "                    'mean_unique_clusters_absolute_acceleration': pd.DataFrame(mean_unique_clusters)['mean_unique_clusters_absolute_acceleration'].mean()\n",
    "                })\n",
    "\n",
    "\n",
    "        gesture.min_samples_and_eps_mean_unique_clusters_df = pd.DataFrame(min_samples_and_eps_mean_unique_clusters)\n",
    "        # print_msg('gesture.min_samples_and_eps_mean_unique_clusters_df: '+str(gesture.min_samples_and_eps_mean_unique_clusters_df))\n",
    "\n",
    "        # select the best eps and min_samples for linear acceleration x\n",
    "        gesture.best_eps_linear_acceleration_x = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_linear_acceleration_x'] - aimed_unique_clusters).abs().argsort()[:1]]['eps'].values[0]\n",
    "        gesture.best_min_samples_linear_acceleration_x = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_linear_acceleration_x'] - aimed_unique_clusters).abs().argsort()[:1]]['min_samples'].values[0]\n",
    "        print_msg(gesture.name+' -> best_eps_linear_acceleration_x.min_samples_and_eps_mean_unique_clusters_df: '+str(gesture.best_eps_linear_acceleration_x))\n",
    "        print_msg(gesture.name+' -> best_min_samples_linear_acceleration_x: '+str(gesture.best_min_samples_linear_acceleration_x))\n",
    "\n",
    "        # select the best eps and min_samples for linear acceleration y\n",
    "        gesture.best_eps_linear_acceleration_y = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_linear_acceleration_y'] - aimed_unique_clusters).abs().argsort()[:1]]['eps'].values[0]\n",
    "        gesture.best_min_samples_linear_acceleration_y = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_linear_acceleration_y'] - aimed_unique_clusters).abs().argsort()[:1]]['min_samples'].values[0]\n",
    "        print_msg(gesture.name+' -> best_eps_linear_acceleration_y: '+str(gesture.best_eps_linear_acceleration_y))\n",
    "        print_msg(gesture.name+' -> best_min_samples_linear_acceleration_y: '+str(gesture.best_min_samples_linear_acceleration_y))\n",
    "\n",
    "        # select the best eps and min_samples for linear acceleration z\n",
    "        gesture.best_eps_linear_acceleration_z = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_linear_acceleration_z'] - aimed_unique_clusters).abs().argsort()[:1]]['eps'].values[0]\n",
    "        gesture.best_min_samples_linear_acceleration_z = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_linear_acceleration_z'] - aimed_unique_clusters).abs().argsort()[:1]]['min_samples'].values[0]\n",
    "        print_msg(gesture.name+' -> best_eps_linear_acceleration_z: '+str(gesture.best_eps_linear_acceleration_z))\n",
    "        print_msg(gesture.name+' -> best_min_samples_linear_acceleration_z: '+str(gesture.best_min_samples_linear_acceleration_z))\n",
    "\n",
    "        # select the best eps and min_samples for absolute acceleration\n",
    "        gesture.best_eps_absolute_acceleration = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_absolute_acceleration'] - aimed_unique_clusters).abs().argsort()[:1]]['eps'].values[0]\n",
    "        gesture.best_min_samples_absolute_acceleration = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_absolute_acceleration'] - aimed_unique_clusters).abs().argsort()[:1]]['min_samples'].values[0]\n",
    "        print_msg(gesture.name+' -> best_eps_absolute_acceleration: '+str(gesture.best_eps_absolute_acceleration))\n",
    "        print_msg(gesture.name+' -> best_min_samples_absolute_acceleration: '+str(gesture.best_min_samples_absolute_acceleration))\n",
    "\n",
    "\n",
    "        # -----------------Selecting the best values of eps and min_samples---------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "Visualize_this_cell = True\n",
    "Visualize_unlabelled_data = True\n",
    "Visualize_labelled_error_data = True\n",
    "Visualize_labelled_correct_data = True\n",
    "# visualizations for final cluster un-labelled data will be stored in the folder cluster un-labelled/[gesture_name]\n",
    "# visualizations for labelled data with error will be stored in the folder cluster labelled error/[gesture_name]\n",
    "# visualizations for labelled data with correction will be stored in the folder cluster labelled correction/[gesture_name]\n",
    "\n",
    "# -----------------labelling the data using the best values of eps and min_samples----------------\n",
    "# label the data using the best values of eps and min_samples for each key in data_wave.data\n",
    "\n",
    "for gesture in [data_circles, data_come_here, data_go_away, data_wave]:\n",
    "    \n",
    "    keys = gesture.data.keys()\n",
    "    for key in keys:\n",
    "        print_sys_msg(key)\n",
    "        print_msg('columns being worked on are: '+str(gesture.data[key].columns))\n",
    "        print_msg('best_eps_linear_acceleration_x: '+str(gesture.best_eps_linear_acceleration_x))\n",
    "        print_msg('best_min_samples_linear_acceleration_x: '+str(gesture.best_min_samples_linear_acceleration_x))\n",
    "        dbscan = DBSCANClustering(gesture.data[key], gesture.best_eps_linear_acceleration_x, gesture.best_min_samples_linear_acceleration_x)\n",
    "        dbscan.cluster()\n",
    "        gesture.data[key]['DBSCAN Clustering linear acceleration x'] = dbscan.clusters[0]\n",
    "        gesture.data[key].final_cluster_densities_linear_acceleration_x = [len(dbscan.clusters[0][dbscan.clusters[0] == cluster]) for cluster in np.unique(dbscan.clusters[0])]\n",
    "        print_msg('final cluster densities linear acceleration x: '+str(gesture.data[key].final_cluster_densities_linear_acceleration_x))      \n",
    "        # calculate and print the mean vlaue on y axis for each cluster in linear acceleration x\n",
    "        for cluster in np.unique(dbscan.clusters[0]):\n",
    "            print_msg('cluster: '+str(cluster)+' mean value: '+str(gesture.data[key][gesture.data[key]['DBSCAN Clustering linear acceleration x'] == cluster]['Linear Acceleration x (m/s^2)'].mean()))\n",
    "        print_msg('best_eps_linear_acceleration_y: '+str(gesture.best_eps_linear_acceleration_y))\n",
    "        print_msg('best_min_samples_linear_acceleration_y: '+str(gesture.best_min_samples_linear_acceleration_y))\n",
    "        dbscan = DBSCANClustering(gesture.data[key], gesture.best_eps_linear_acceleration_y, gesture.best_min_samples_linear_acceleration_y)\n",
    "        dbscan.cluster()\n",
    "        gesture.data[key]['DBSCAN Clustering linear acceleration y'] = dbscan.clusters[1]\n",
    "        gesture.data[key].final_cluster_densities_linear_acceleration_y = [len(dbscan.clusters[1][dbscan.clusters[1] == cluster]) for cluster in np.unique(dbscan.clusters[1])]\n",
    "        print_msg('final cluster densities linear acceleration y: '+str(gesture.data[key].final_cluster_densities_linear_acceleration_y))\n",
    "        for cluster in np.unique(dbscan.clusters[1]):\n",
    "            print_msg('cluster: '+str(cluster)+' mean value: '+str(gesture.data[key][gesture.data[key]['DBSCAN Clustering linear acceleration y'] == cluster]['Linear Acceleration y (m/s^2)'].mean()))\n",
    "        print_msg('best_eps_linear_acceleration_z: '+str(gesture.best_eps_linear_acceleration_z))\n",
    "        print_msg('best_min_samples_linear_acceleration_z: '+str(gesture.best_min_samples_linear_acceleration_z))\n",
    "        dbscan = DBSCANClustering(gesture.data[key], gesture.best_eps_linear_acceleration_z, gesture.best_min_samples_linear_acceleration_z)\n",
    "        dbscan.cluster()\n",
    "        gesture.data[key]['DBSCAN Clustering linear acceleration z'] = dbscan.clusters[2]\n",
    "        gesture.data[key].final_cluster_densities_linear_acceleration_z = [len(dbscan.clusters[2][dbscan.clusters[2] == cluster]) for cluster in np.unique(dbscan.clusters[2])]\n",
    "        print_msg('final cluster densities linear acceleration z: '+str(gesture.data[key].final_cluster_densities_linear_acceleration_z))\n",
    "        for cluster in np.unique(dbscan.clusters[2]):\n",
    "            print_msg('cluster: '+str(cluster)+' mean value: '+str(gesture.data[key][gesture.data[key]['DBSCAN Clustering linear acceleration z'] == cluster]['Linear Acceleration z (m/s^2)'].mean()))\n",
    "        print_msg('best_eps_absolute_acceleration: '+str(gesture.best_eps_absolute_acceleration))\n",
    "        print_msg('best_min_samples_absolute_acceleration: '+str(gesture.best_min_samples_absolute_acceleration))\n",
    "        dbscan = DBSCANClustering(gesture.data[key], gesture.best_eps_absolute_acceleration, gesture.best_min_samples_absolute_acceleration)\n",
    "        dbscan.cluster()\n",
    "        gesture.data[key]['DBSCAN Clustering absolute acceleration'] = dbscan.clusters[3]\n",
    "        gesture.data[key].final_cluster_densities_absolute_acceleration = [len(dbscan.clusters[3][dbscan.clusters[3] == cluster]) for cluster in np.unique(dbscan.clusters[3])]\n",
    "        print_msg('final cluster densities absolute acceleration: '+str(gesture.data[key].final_cluster_densities_absolute_acceleration))\n",
    "        for cluster in np.unique(dbscan.clusters[3]):\n",
    "            print_msg('cluster: '+str(cluster)+' mean value: '+str(gesture.data[key][gesture.data[key]['DBSCAN Clustering absolute acceleration'] == cluster]['Absolute acceleration (m/s^2)'].mean()))\n",
    "        \n",
    "\n",
    "        if Visualize_this_cell and Visualize_unlabelled_data: # set to True to visualize the final_un-labelled data\n",
    "            if PLOT_ALL == True or PLOT == True:\n",
    "                dv = DataVisualization(gesture.data[key]['Time (s)'])\n",
    "                folder_name = 'cluster final un-labelled/'+gesture.name\n",
    "                gesture.create_folder(folder_name)\n",
    "                if PLOT_ALL == True or (gesture.name in FOLLOW_GESTURE and key in FOLLOW_KEY):\n",
    "                    dv.switch_to_seaborn(False)\n",
    "                    dv.set_grid_params(2, 2, 20, 10, gesture.name+' -> '+key+' -> DBSCAN Clustering')\n",
    "                    dv.add_data(gesture.data[key].iloc[:, 1])\n",
    "                    dv.plot_grid_2d(0, 0, 'scatter', 'Time (s)', gesture.data[key].columns[1], gesture.data[key].columns[1] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_x)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_x), c=gesture.data[key]['DBSCAN Clustering linear acceleration x'])\n",
    "                    dv.plot_grid_2d(0, 0, 'line' , 'Time (s)', gesture.data[key].columns[1], gesture.data[key].columns[1] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_x)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_x), c='red', thickness=0.3)\n",
    "                    dv.add_data(gesture.data[key].iloc[:, 2])\n",
    "                    dv.plot_grid_2d(0, 1, 'scatter', 'Time (s)', gesture.data[key].columns[2], gesture.data[key].columns[2] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_y)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_y), c=gesture.data[key]['DBSCAN Clustering linear acceleration y'])\n",
    "                    dv.plot_grid_2d(0, 1, 'line' , 'Time (s)', gesture.data[key].columns[2], gesture.data[key].columns[2] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_y)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_y), c='red', thickness=0.3)\n",
    "                    dv.add_data(gesture.data[key].iloc[:, 3])\n",
    "                    dv.plot_grid_2d(1, 0, 'scatter', 'Time (s)', gesture.data[key].columns[3], gesture.data[key].columns[3] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_z)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_z), c=gesture.data[key]['DBSCAN Clustering linear acceleration z'])\n",
    "                    dv.plot_grid_2d(1, 0, 'line' , 'Time (s)', gesture.data[key].columns[3], gesture.data[key].columns[3] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_z)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_z), c='red', thickness=0.3)\n",
    "                    dv.add_data(gesture.data[key].iloc[:, 4])\n",
    "                    dv.plot_grid_2d(1, 1, 'scatter', 'Time (s)', gesture.data[key].columns[4], gesture.data[key].columns[4] + ' -> eps: '+str(gesture.best_eps_absolute_acceleration)+' min_samples: '+str(gesture.best_min_samples_absolute_acceleration), c=gesture.data[key]['DBSCAN Clustering absolute acceleration'])\n",
    "                    dv.plot_grid_2d(1, 1, 'line' , 'Time (s)', gesture.data[key].columns[4], gesture.data[key].columns[4] + ' -> eps: '+str(gesture.best_eps_absolute_acceleration)+' min_samples: '+str(gesture.best_min_samples_absolute_acceleration), c='red', thickness=0.3)\n",
    "                    dv.show_plot()\n",
    "                    dv.switch_to_seaborn(False)\n",
    "                    dv.fig.savefig(folder_name+'/'+key+'.png')\n",
    "                    \n",
    "        # label the data by setting a new column in the data with the name 'Gesture' set to [1 for circular motion, 2 for come here, 3 for go away, 4 for wave] if the data is clustered at -1 in any of the 4 ways, else set the data to 0.\n",
    "        gesture.data[key]['Gesture'] = 0\n",
    "        gesture.data[key].loc[(gesture.data[key]['DBSCAN Clustering linear acceleration x'] == -1) | (gesture.data[key]['DBSCAN Clustering linear acceleration y'] == -1) | (gesture.data[key]['DBSCAN Clustering linear acceleration z'] == -1) | (gesture.data[key]['DBSCAN Clustering absolute acceleration'] == -1), 'Gesture'] = 1 if gesture.name == 'circles' else 2 if gesture.name == 'come here' else 3 if gesture.name == 'go away' else 4\n",
    "        \n",
    "        # visualize the labelled data with a new column 'Gesture' overlayed on each of the 4 ways linear acceleration x, linear acceleration y, linear acceleration z, and absolute acceleration.\n",
    "        # note that the this data will have a lot of 0s in between the gestures, hence we'll interpolate the data to remove the 0s in between the gestures.\n",
    "        if Visualize_this_cell and Visualize_labelled_error_data:\n",
    "            if PLOT_ALL == True or PLOT == True:\n",
    "                dv = DataVisualization(gesture.data[key]['Time (s)'])\n",
    "                folder_name = 'cluster labelled error/'+gesture.name\n",
    "                gesture.create_folder(folder_name)\n",
    "                if PLOT_ALL == True or (gesture.name in FOLLOW_GESTURE and key in FOLLOW_KEY):\n",
    "                    dv.switch_to_seaborn(False)\n",
    "                    dv.set_grid_params(2, 2, 20, 10, gesture.name+' -> '+key+' -> Labelled with errors')\n",
    "                    dv.add_data(gesture.data[key].iloc[:, 1])\n",
    "                    dv.plot_grid_2d(0, 0, 'line', 'Time (s)', gesture.data[key].columns[1], gesture.data[key].columns[1] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_x)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_x), c='red')\n",
    "                    dv.add_data(gesture.data[key]['Gesture'])\n",
    "                    dv.plot_grid_2d(0, 0, 'line', 'Time (s)', gesture.data[key].columns[1], 'Gesture', c='blue')\n",
    "                    dv.add_data(gesture.data[key].iloc[:, 2])\n",
    "                    dv.plot_grid_2d(0, 1, 'line', 'Time (s)', gesture.data[key].columns[2], gesture.data[key].columns[2] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_y)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_y), c='red')\n",
    "                    dv.add_data(gesture.data[key]['Gesture'])\n",
    "                    dv.plot_grid_2d(0, 1, 'line', 'Time (s)', gesture.data[key].columns[2], 'Gesture', c='blue')\n",
    "                    dv.add_data(gesture.data[key].iloc[:, 3])\n",
    "                    dv.plot_grid_2d(1, 0, 'line', 'Time (s)', gesture.data[key].columns[3], gesture.data[key].columns[3] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_z)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_z), c='red')\n",
    "                    dv.add_data(gesture.data[key]['Gesture'])\n",
    "                    dv.plot_grid_2d(1, 0, 'line', 'Time (s)', gesture.data[key].columns[3], 'Gesture', c='blue')\n",
    "                    dv.add_data(gesture.data[key].iloc[:, 4])\n",
    "                    dv.plot_grid_2d(1, 1, 'line', 'Time (s)', gesture.data[key].columns[4], gesture.data[key].columns[4] + ' -> eps: '+str(gesture.best_eps_absolute_acceleration)+' min_samples: '+str(gesture.best_min_samples_absolute_acceleration), c='red')\n",
    "                    dv.add_data(gesture.data[key]['Gesture'])\n",
    "                    dv.plot_grid_2d(1, 1, 'line', 'Time (s)', gesture.data[key].columns[4], 'Gesture', c='blue')\n",
    "                    dv.show_plot()\n",
    "                    dv.switch_to_seaborn(False)\n",
    "                    dv.fig.savefig(folder_name+'/'+key+'.png')\n",
    "        # interpolate the data to remove the 0s in between the gestures.\n",
    "        gesture.data[key]['Gesture'] = gesture.data[key]['Gesture'].replace(0, np.nan)\n",
    "        gesture.data[key]['Gesture'] = gesture.data[key]['Gesture'].interpolate(limit=interpolation_limit, limit_direction='both')\n",
    "        gesture.data[key]['Gesture'] = gesture.data[key]['Gesture'].fillna(0)\n",
    "        gesture.data[key]['Gesture'] = gesture.data[key]['Gesture'].astype(int) \n",
    "        # visualize the labelled data with a new column 'Gesture' overlayed on each of the 4 ways linear acceleration x, linear acceleration y, linear acceleration z, and absolute acceleration.\n",
    "        if Visualize_this_cell and Visualize_labelled_correct_data:\n",
    "            if PLOT_ALL == True or PLOT == True:\n",
    "                dv = DataVisualization(gesture.data[key]['Time (s)'])\n",
    "                folder_name = 'cluster labelled correction/'+gesture.name\n",
    "                gesture.create_folder(folder_name)\n",
    "                if PLOT_ALL == True or (gesture.name in FOLLOW_GESTURE and key in FOLLOW_KEY):\n",
    "                    dv.switch_to_seaborn(False)\n",
    "    \n",
    "                    dv.set_grid_params(2, 2, 20, 10, gesture.name+' -> '+key+' -> Corrected Labels')\n",
    "    \n",
    "                    dv.add_data(gesture.data[key].iloc[:, 1])\n",
    "                    dv.plot_grid_2d(0, 0, 'line', 'Time (s)', gesture.data[key].columns[1], gesture.data[key].columns[1] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_x)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_x), c='red')\n",
    "                    dv.add_data(gesture.data[key]['Gesture'])\n",
    "                    dv.plot_grid_2d(0, 0, 'line', 'Time (s)', gesture.data[key].columns[1], 'Gesture', c='blue')\n",
    "    \n",
    "                    dv.add_data(gesture.data[key].iloc[:, 2])\n",
    "                    dv.plot_grid_2d(0, 1, 'line', 'Time (s)', gesture.data[key].columns[2], gesture.data[key].columns[2] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_y)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_y), c='red')\n",
    "                    dv.add_data(gesture.data[key]['Gesture'])\n",
    "                    dv.plot_grid_2d(0, 1, 'line', 'Time (s)', gesture.data[key].columns[2], 'Gesture', c='blue')\n",
    "    \n",
    "                    dv.add_data(gesture.data[key].iloc[:, 3])\n",
    "                    dv.plot_grid_2d(1, 0, 'line', 'Time (s)', gesture.data[key].columns[3], gesture.data[key].columns[3] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_z)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_z), c='red')\n",
    "                    dv.add_data(gesture.data[key]['Gesture'])\n",
    "                    dv.plot_grid_2d(1, 0, 'line', 'Time (s)', gesture.data[key].columns[3], 'Gesture', c='blue')\n",
    "    \n",
    "                    dv.add_data(gesture.data[key].iloc[:, 4])\n",
    "                    dv.plot_grid_2d(1, 1, 'line', 'Time (s)', gesture.data[key].columns[4], gesture.data[key].columns[4] + ' -> eps: '+str(gesture.best_eps_absolute_acceleration)+' min_samples: '+str(gesture.best_min_samples_absolute_acceleration), c='red')\n",
    "                    dv.add_data(gesture.data[key]['Gesture'])\n",
    "                    dv.plot_grid_2d(1, 1, 'line', 'Time (s)', gesture.data[key].columns[4], 'Gesture', c='blue')\n",
    "    \n",
    "                    dv.show_plot()\n",
    "    \n",
    "                    dv.switch_to_seaborn(False)\n",
    "                    dv.fig.savefig(folder_name+'/'+key+'.png')\n",
    "        # # save the labelled data to a xlsx file\n",
    "        # gesture.data[key].to_excel('cluster labelled/'+gesture.name+'/'+key+'.xlsx')\n",
    "# -----------------labelling the data using the best values of eps and min_samples----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "Visualize_this_cell = True\n",
    "# visualizations for final cluster un-labelled data will be stored in the folder cluster label std count/[gesture_name]\n",
    "for gesture in [data_circles, data_come_here, data_go_away, data_wave]:\n",
    "    \n",
    "    folder_name = 'cluster label std corrected/'+gesture.name\n",
    "    gesture.create_folder(folder_name)\n",
    "\n",
    "    keys = gesture.data.keys()\n",
    "    # Since there are outliers in the gestures, we will remove them by checking the size of each gesture and if a particular gesture is on the far end of the normalized size, then we'll eliminate it.\n",
    "\n",
    "    # we collect the size of each gesture across all keys in a list and then we'll calculate the mean and standard deviation of the list. We'll then set the lower and upper limit for the size to be and set the gestures to 0 which don't fall in the range.\n",
    "    gesture_sizes = []\n",
    "    for key in keys:\n",
    "        temp_size = []\n",
    "        temp_count = 0\n",
    "        count_flag = 0 # 1 to indicate that a counter has begone\n",
    "        for i in range(len(gesture.data[key]['Gesture'])):\n",
    "            if gesture.data[key]['Gesture'][i] != 0:\n",
    "                temp_count += 1\n",
    "                count_flag = 1\n",
    "            if gesture.data[key]['Gesture'][i] == 0 and count_flag == 1:\n",
    "                temp_size.append(temp_count)\n",
    "                temp_count = 0\n",
    "                count_flag = 0\n",
    "        gesture_sizes += temp_size\n",
    "    \n",
    "    print_msg('gesture_sizes: '+str(gesture_sizes))\n",
    "\n",
    "\n",
    "    # eliminating, from bottom and top, 5% of the gestures using standard deviation\n",
    "    # changing a the value in gesture_sizes_lower_limit and gesture_sizes_upper_limit will change the number of gestures that are accepted from within the data.\n",
    "    # 1     ->  68.2%\n",
    "    # 1.5   ->  86.6%\n",
    "    # 2     ->  95.4%\n",
    "    # 2.5   ->  98.8%\n",
    "    # 3     ->  99.7%\n",
    "    gesture_sizes = np.array(gesture_sizes)\n",
    "    gesture_sizes_mean = gesture_sizes.mean()\n",
    "    gesture_sizes_std = np.sqrt(np.mean((gesture_sizes - gesture_sizes_mean)**2))\n",
    "    gesture_sizes_lower_limit = gesture_sizes_mean - 2 * gesture_sizes_std \n",
    "    gesture_sizes_upper_limit = gesture_sizes_mean + 2 * gesture_sizes_std\n",
    "    print_msg('gesture_sizes_mean: '+str(gesture_sizes_mean))\n",
    "    # standard deviation takes in the square root of the mean of the squared differences between the data and the mean. In this case we are using the mean of the squared differences between the data and the mean as the standard deviation.\n",
    "    # this elemintates the outliers in the data that occur due to the noise.\n",
    "\n",
    "    # set the gestures to 0 which don't fall in the range.\n",
    "    for key in keys:\n",
    "        temp_count = 0\n",
    "        count_flag = 0\n",
    "        for i in range(len(gesture.data[key]['Gesture'])):\n",
    "            if gesture.data[key]['Gesture'][i] != 0:\n",
    "                temp_count += 1\n",
    "                count_flag = 1\n",
    "            if gesture.data[key]['Gesture'][i] == 0 and count_flag == 1:\n",
    "                if temp_count < gesture_sizes_lower_limit or temp_count > gesture_sizes_upper_limit:\n",
    "                    gesture.data[key]['Gesture'][i-temp_count:i] = 0\n",
    "                temp_count = 0\n",
    "                count_flag = 0\n",
    "        print_sys_msg('key: '+key+' gesture_sizes_lower_limit: '+str(gesture_sizes_lower_limit)+' gesture_sizes_upper_limit: '+str(gesture_sizes_upper_limit))\n",
    "        print_msg('std correction for key '+key+' completed.')\n",
    "        # visualize the labelled data with a new column 'Gesture' overlayed on each of the 4 ways linear acceleration x, linear acceleration y, linear acceleration z, and absolute acceleration.\n",
    "        if Visualize_this_cell:\n",
    "            if PLOT_ALL == True or PLOT == True:\n",
    "                dv = DataVisualization(gesture.data[key]['Time (s)'])\n",
    "                if PLOT_ALL == True or (gesture.name in FOLLOW_GESTURE and key in FOLLOW_KEY) or True:\n",
    "                    dv.switch_to_seaborn(False)\n",
    "                    dv.set_grid_params(2, 2, 20, 10, gesture.name+' -> '+key+' -> DBSCAN Clustering')\n",
    "                    dv.add_data(gesture.data[key].iloc[:, 1])\n",
    "                    dv.plot_grid_2d(0, 0, 'line', 'Time (s)', gesture.data[key].columns[1], gesture.data[key].columns[1] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_x)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_x), c='red')\n",
    "                    dv.add_data(gesture.data[key]['Gesture'])\n",
    "                    dv.plot_grid_2d(0, 0, 'line', 'Time (s)', gesture.data[key].columns[1], 'Gesture', c='blue')\n",
    "                    dv.add_data(gesture.data[key].iloc[:, 2])\n",
    "                    dv.plot_grid_2d(0, 1, 'line', 'Time (s)', gesture.data[key].columns[2], gesture.data[key].columns[2] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_y)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_y), c='red')\n",
    "                    dv.add_data(gesture.data[key]['Gesture'])\n",
    "                    dv.plot_grid_2d(0, 1, 'line', 'Time (s)', gesture.data[key].columns[2], 'Gesture', c='blue')\n",
    "                    dv.add_data(gesture.data[key].iloc[:, 3])\n",
    "                    dv.plot_grid_2d(1, 0, 'line', 'Time (s)', gesture.data[key].columns[3], gesture.data[key].columns[3] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_z)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_z), c='red')\n",
    "                    dv.add_data(gesture.data[key]['Gesture'])\n",
    "                    dv.plot_grid_2d(1, 0, 'line', 'Time (s)', gesture.data[key].columns[3], 'Gesture', c='blue')\n",
    "                    dv.add_data(gesture.data[key].iloc[:, 4])\n",
    "                    dv.plot_grid_2d(1, 1, 'line', 'Time (s)', gesture.data[key].columns[4], gesture.data[key].columns[4] + ' -> eps: '+str(gesture.best_eps_absolute_acceleration)+' min_samples: '+str(gesture.best_min_samples_absolute_acceleration), c='red')\n",
    "                    dv.add_data(gesture.data[key]['Gesture'])\n",
    "                    dv.plot_grid_2d(1, 1, 'line', 'Time (s)', gesture.data[key].columns[4], 'Gesture', c='blue')\n",
    "                    dv.show_plot()\n",
    "                    dv.switch_to_seaborn(False)\n",
    "                    dv.fig.savefig(folder_name+'/'+key+'.png')\n",
    "        \n",
    "        gesture.data[key].to_excel(folder_name+'/'+key+'.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminating data that have exceptionally large amount of noise. These data were collected in an exaggerated manner to understand the effects on the final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class sliding time window. it will create frames of dataframes based on the sliding time window size. \n",
    "# window_size -> size of the window which represents the number of rows in the dataframe\n",
    "# step_size -> size of the step which represents the number of rows to move the window by \n",
    "# data -> dataframe which will be used to create frames\n",
    "# create_frames -> function to create frames of dataframes based on the sliding time window size\n",
    "# get_frames -> function to get the frames\n",
    "\n",
    "\n",
    "class SlidingTimeWindow:\n",
    "    knn_tried_neighbours = []\n",
    "    knn_scores = []\n",
    "    knn_accuracy = []\n",
    "\n",
    "    def __init__(self, data, window_size, step_size):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        self.frames = []\n",
    "    \n",
    "    def create_frames(self):\n",
    "        for i in range(0, len(self.data), self.step_size):\n",
    "            if i + self.window_size < len(self.data):\n",
    "                self.frames.append(self.data.iloc[i:i+self.window_size])\n",
    "            else:\n",
    "                self.frames.append(self.data.iloc[i:])\n",
    "        # all frames must have the same number of rows. Hence dropping frames that don't have the same number of rows as the window size.\n",
    "        self.frames = [frame for frame in self.frames if len(frame) == self.window_size]\n",
    "        \n",
    "\n",
    "    def check_one_complete_gesture(self, frame):\n",
    "        # for each unique gesture in the frame, check if the gesture has one complete gesture in the frame. that is, in that frame, the unique value is surrounded by a value other than itself.\n",
    "        # for this we'll make a 2d stack. we'll traverse across 'Gesture'. we'll note and count the value we are following and upon encountring a different value we'll push the value and it's count to the stack. if the value does not encounter it's end by the end of the frame, it's not pushed to the stack.\n",
    "\n",
    "        stack = []  # This will be our 2D stack to store (value, count)\n",
    "        current_value = None\n",
    "        count = 0\n",
    "\n",
    "        for gesture in frame['Gesture']:\n",
    "            if gesture == current_value:\n",
    "                count += 1\n",
    "            else:\n",
    "                if current_value is not None:\n",
    "                    stack.append((current_value, count))\n",
    "                current_value = gesture\n",
    "                count = 1\n",
    "\n",
    "        # Append the last value and count to the stack if end of DataFrame is reached\n",
    "        if current_value is not None:\n",
    "            stack.append((current_value, count))\n",
    "\n",
    "        # Check for a single complete gesture surrounded by zeros\n",
    "        gesture_found = None\n",
    "        for i in range(1, len(stack) - 1):\n",
    "            if stack[i-1][0] == 0 and stack[i+1][0] == 0 and stack[i][0] != 0:\n",
    "                # Check if there's already a gesture found or if this gesture repeats\n",
    "                if gesture_found is not None:\n",
    "                    return stack, 0  # More than one gesture found, or not properly isolated\n",
    "                gesture_found = stack[i][0]\n",
    "\n",
    "        if gesture_found is None:\n",
    "            return stack, 0  # No gesture found\n",
    "        return stack, gesture_found\n",
    "\n",
    "\n",
    "    def get_frames(self):\n",
    "        return self.frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "Visualize_this_cell = True\n",
    "# choosing a value for gesture.stw and gesture.stws such that each frame that is made using it only holds one gesture\n",
    "\n",
    "# we will now determine the size of gesture.stw (sliding time window) and gesture.stws (sliding time window step) such that each window has at most 1 gesture but each gesture should completely fit in atleast one window.\n",
    "\n",
    "# we'll first calculate the maximum size of a gesture among all keys of all gestures \n",
    "\n",
    "maximum_gesture_size = 0\n",
    "\n",
    "for gesture in [data_circles, data_come_here, data_go_away, data_wave]:\n",
    "    keys = gesture.data.keys()\n",
    "    gesture_sizes = []\n",
    "    for key in keys:\n",
    "        temp_size = []\n",
    "        temp_count = 0\n",
    "        count_flag = 0 # 1 to indicate that a counter has begone\n",
    "        for i in range(len(gesture.data[key]['Gesture'])):\n",
    "            if gesture.data[key]['Gesture'][i] != 0:\n",
    "                temp_count += 1\n",
    "                count_flag = 1\n",
    "            if gesture.data[key]['Gesture'][i] == 0 and count_flag == 1:\n",
    "                temp_size.append(temp_count)\n",
    "                temp_count = 0\n",
    "                count_flag = 0\n",
    "        gesture_sizes += temp_size\n",
    "    if max(gesture_sizes) > maximum_gesture_size:\n",
    "        maximum_gesture_size = max(gesture_sizes)\n",
    "\n",
    "print_msg('maximum_gesture_size: '+str(maximum_gesture_size))\n",
    "\n",
    "# the window size will be of the size of the maximum gesture size + 20 each side to make sure that the gesture fits in the window\n",
    "\n",
    "stw_size = maximum_gesture_size + 20\n",
    "\n",
    "# for the preduction model, we'll be creating 3 dataset. each dataset will have stack of frames of all gestures but the step size of the frame will be 25%, 50%, and 75% of the window size.\n",
    "\n",
    "# joining data of all gestures in one dataframe\n",
    "data_all = pd.DataFrame()\n",
    "for gesture in [data_circles, data_come_here, data_go_away, data_wave]:\n",
    "    keys = gesture.data.keys()\n",
    "    for key in keys:\n",
    "        data_all = pd.concat([data_all, gesture.data[key]], axis=0)\n",
    "\n",
    "print_sys_msg('data_all shape: '+str(data_all.shape))\n",
    "print_sys_msg('data_all columns: '+str(data_all.columns))\n",
    "\n",
    "\n",
    "# create a stack of frames of all gestures with a step size of 25% of the window size named data_all_25\n",
    "stw_25 = SlidingTimeWindow(data_all, stw_size, int(stw_size * 0.25))\n",
    "stw_25.create_frames()\n",
    "print_msg('number of frames in data_all_25: '+str(len(stw_25.frames)))\n",
    "for each in stw_25.frames:\n",
    "    stack, gesture_found = stw_25.check_one_complete_gesture(each)\n",
    "    print_sys_msg(' for frame gesture found: '+str(gesture_found))\n",
    "\n",
    "    \n",
    "# create a stack of frames of all gestures with a step size of 50% of the window size named data_all_50\n",
    "stw_50 = SlidingTimeWindow(data_all, stw_size, int(stw_size * 0.50))\n",
    "stw_50.create_frames()\n",
    "print_msg('number of frames in data_all_50: '+str(len(stw_50.frames)))\n",
    "\n",
    "# create a stack of frames of all gestures with a step size of 75% of the window size named data_all_75\n",
    "stw_75 = SlidingTimeWindow(data_all, stw_size, int(stw_size * 0.75))\n",
    "stw_75.create_frames()         \n",
    "print_msg('number of frames in data_all_75: '+str(len(stw_75.frames)))            \n",
    "\n",
    "# Visualizing and saving each frame in the data_all_25, data_all_50, and data_all_75 datasets having linear acceleration x, linear acceleration y, linear acceleration z, and absolute acceleration\n",
    "if Visualize_this_cell:\n",
    "    for each in [stw_25, stw_50, stw_75]:\n",
    "        if os.path.exists('frames/') == False:\n",
    "            os.mkdir('frames/')\n",
    "        if os.path.exists('frames/'+str(len(each.frames))) == False:\n",
    "            os.mkdir('frames/'+str(len(each.frames)))\n",
    "\n",
    "        # for i in range(len(each)):\n",
    "        for i in range(100):\n",
    "            stack, gesture_found = each.check_one_complete_gesture(each.frames[i])\n",
    "            dv = DataVisualization(each.frames[i]['Time (s)'])\n",
    "            dv.switch_to_seaborn(False)\n",
    "            dv.set_grid_params(2, 2, 20, 10, 'Frame '+str(i)+' -> '+str(gesture_found) if gesture_found != 0 else 'Frame '+str(i)+' -> No gesture found')\n",
    "            dv.add_data(each.frames[i].iloc[:, 1])\n",
    "            dv.plot_grid_2d(0, 0, 'line', 'Time (s)', each.frames[i].columns[1], each.frames[i].columns[1], c='red')\n",
    "            dv.add_data(each.frames[i].iloc[:, 9])\n",
    "            dv.plot_grid_2d(0, 0, 'line', 'Time (s)', each.frames[i].columns[9], each.frames[i].columns[9], c='blue')\n",
    "            dv.add_data(each.frames[i].iloc[:, 2])\n",
    "            dv.plot_grid_2d(0, 1, 'line', 'Time (s)', each.frames[i].columns[2], each.frames[i].columns[2], c='red')\n",
    "            dv.add_data(each.frames[i].iloc[:, 9])\n",
    "            dv.plot_grid_2d(0, 1, 'line', 'Time (s)', each.frames[i].columns[9], each.frames[i].columns[9], c='blue')\n",
    "            dv.add_data(each.frames[i].iloc[:, 3])\n",
    "            dv.plot_grid_2d(1, 0, 'line', 'Time (s)', each.frames[i].columns[3], each.frames[i].columns[3], c='red')\n",
    "            dv.add_data(each.frames[i].iloc[:, 9])\n",
    "            dv.plot_grid_2d(1, 0, 'line', 'Time (s)', each.frames[i].columns[9], each.frames[i].columns[9], c='blue')\n",
    "            dv.add_data(each.frames[i].iloc[:, 4])\n",
    "            dv.plot_grid_2d(1, 1, 'line', 'Time (s)', each.frames[i].columns[4], each.frames[i].columns[4], c='red')\n",
    "            dv.add_data(each.frames[i].iloc[:, 9])\n",
    "            dv.plot_grid_2d(1, 1, 'line', 'Time (s)', each.frames[i].columns[9], each.frames[i].columns[9], c='blue')\n",
    "            dv.show_plot()\n",
    "            dv.switch_to_seaborn(False)\n",
    "\n",
    "            dv.fig.savefig('frames/'+str(len(each.frames))+'/'+str(i)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly applying random forest classifier to the data\n",
    "random_forest_tried_estimators = []\n",
    "random_forest_scores = []\n",
    "random_forest_accuracy = []\n",
    "for each in [stw_25, stw_50, stw_75]:\n",
    "    print_sys_msg(f'each: {each}')\n",
    "\n",
    "    rf_data_list = []  # List to hold all data\n",
    "    frame_count = 0\n",
    "\n",
    "    for each_frame in each.frames:\n",
    "        stack, gesture_found = each.check_one_complete_gesture(each_frame)\n",
    "        each_frame = each_frame[['Linear Acceleration x (m/s^2)', 'Linear Acceleration y (m/s^2)', \n",
    "                                 'Linear Acceleration z (m/s^2)', 'Absolute acceleration (m/s^2)']]\n",
    "        \n",
    "        features = each_frame.values.flatten()\n",
    "\n",
    "        rf_data_list.append(np.append(features, gesture_found))\n",
    "        frame_count += 1\n",
    "\n",
    "    # Use the length of the first item in rf_data_list to set column names\n",
    "    if rf_data_list:\n",
    "        column_names = [f'Feature {i}' for i in range(len(rf_data_list[0]) - 1)] + ['Gesture']\n",
    "        rf_data = pd.DataFrame(rf_data_list, columns=column_names)\n",
    "    else:\n",
    "        print_sys_msg(\"No data processed.\")\n",
    "\n",
    "    # Remove NaN values\n",
    "    rf_data.dropna(inplace=True)\n",
    "\n",
    "    # Data standardization and normalization\n",
    "    scaler = StandardScaler()\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    features = rf_data.columns[:-1]  # Exclude 'Gesture' column for scaling\n",
    "\n",
    "    rf_data[features] = scaler.fit_transform(rf_data[features])\n",
    "    rf_data[features] = min_max_scaler.fit_transform(rf_data[features])\n",
    "\n",
    "    # Balance the dataset\n",
    "    min_samples = rf_data['Gesture'].value_counts().min()\n",
    "    rf_data = rf_data.groupby('Gesture').apply(lambda x: x.sample(min_samples)).reset_index(drop=True)\n",
    "    print_sys_msg(f'min_samples: {min_samples}')\n",
    "\n",
    "    # printing randomly selected 50 rows of the data\n",
    "    print_sys_msg(f'rf_data sample: {rf_data.sample(50)}')\n",
    "\n",
    "    # Split the data\n",
    "    X = rf_data.iloc[:, :-1]\n",
    "    y = rf_data['Gesture']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Grid search for Random Forest hyperparameters\n",
    "    param_grid = {'n_estimators': [100, 200, 300]}\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_values = grid_search.best_params_\n",
    "    print('Best values:', best_values)\n",
    "\n",
    "    # Train and test Random Forest model\n",
    "    rf = RandomForestClassifier(n_estimators=best_values['n_estimators'])\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy with cross-validation\n",
    "    scores = cross_val_score(rf, X, y, cv=5)\n",
    "    accuracy = scores.mean()\n",
    "    print('Accuracy:', accuracy)\n",
    "\n",
    "    # Store results in each object\n",
    "    random_forest_tried_estimators.append(best_values['n_estimators'])\n",
    "    random_forest_scores.append(scores)\n",
    "    random_forest_accuracy.append(accuracy)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f'Random Forest with step size {25 * (i + 1)}%')\n",
    "    print(f'Tried estimators: {random_forest_tried_estimators[i]}')\n",
    "    print(f'Scores: {random_forest_scores[i]}')\n",
    "    print(f'Accuracy: {random_forest_accuracy[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly applying SVM classifier to the data\n",
    "svm_tried_kernels = []\n",
    "svm_scores = []\n",
    "svm_accuracy = []\n",
    "\n",
    "for each in [stw_25, stw_50, stw_75]:\n",
    "    print_sys_msg(f'each: {each}')\n",
    "\n",
    "    svm_data_list = []  # List to hold all data\n",
    "    frame_count = 0\n",
    "\n",
    "    for each_frame in each.frames:\n",
    "        stack, gesture_found = each.check_one_complete_gesture(each_frame)\n",
    "        each_frame = each_frame[['Linear Acceleration x (m/s^2)', 'Linear Acceleration y (m/s^2)', \n",
    "                                 'Linear Acceleration z (m/s^2)', 'Absolute acceleration (m/s^2)']]\n",
    "        \n",
    "        features = each_frame.values.flatten()\n",
    "\n",
    "        svm_data_list.append(np.append(features, gesture_found))\n",
    "        frame_count += 1\n",
    "\n",
    "    # Use the length of the first item in svm_data_list to set column names\n",
    "    if svm_data_list:\n",
    "        column_names = [f'Feature {i}' for i in range(len(svm_data_list[0]) - 1)] + ['Gesture']\n",
    "        svm_data = pd.DataFrame(svm_data_list, columns=column_names)\n",
    "    else:\n",
    "        print_sys_msg(\"No data processed.\")\n",
    "\n",
    "\n",
    "    # Remove NaN values\n",
    "    svm_data.dropna(inplace=True)\n",
    "\n",
    "    # Data standardization and normalization\n",
    "    scaler = StandardScaler()\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    features = svm_data.columns[:-1]  # Exclude 'Gesture' column for scaling\n",
    "\n",
    "    svm_data[features] = scaler.fit_transform(svm_data[features])\n",
    "    svm_data[features] = min_max_scaler.fit_transform(svm_data[features])\n",
    "\n",
    "    # Balance the dataset\n",
    "    min_samples = svm_data['Gesture'].value_counts().min()\n",
    "    svm_data = svm_data.groupby('Gesture').apply(lambda x: x.sample(min_samples)).reset_index(drop=True)\n",
    "    print_sys_msg(f'min_samples: {min_samples}')\n",
    "\n",
    "    # Split the data\n",
    "    X = svm_data.iloc[:, :-1]\n",
    "    y = svm_data['Gesture']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Grid search for SVM hyperparameters\n",
    "    param_grid = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "    grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_values = grid_search.best_params_\n",
    "    print('Best values:', best_values)\n",
    "\n",
    "    # Train and test SVM model\n",
    "    svm = SVC(kernel=best_values['kernel'])\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy with cross-validation\n",
    "    scores = cross_val_score(svm, X, y, cv=5)\n",
    "    accuracy = scores.mean()\n",
    "    print('Accuracy:', accuracy)\n",
    "\n",
    "    # Store results in each object\n",
    "    svm_tried_kernels.append(best_values['kernel'])\n",
    "    svm_scores.append(scores)\n",
    "    svm_accuracy.append(accuracy)\n",
    "\n",
    "for i in range(3):\n",
    "    print_msg(f'SVM with step size {25 * (i + 1)}%')\n",
    "    print_msg(f'Tried kernels: {svm_tried_kernels[i]}')\n",
    "    print_msg(f'Scores: {svm_scores[i]}')\n",
    "    print_msg(f'Accuracy: {svm_accuracy[i]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLY KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similary apply KNN\n",
    "\n",
    "knn_tried_neighbours = []\n",
    "knn_scores = []\n",
    "knn_accuracy = []\n",
    "\n",
    "for each in [stw_25, stw_50, stw_75]:\n",
    "\n",
    "    knn_data_list = []  # List to hold all data\n",
    "    frame_count = 0\n",
    "\n",
    "    for each_frame in each.frames:\n",
    "        stack, gesture_found = each.check_one_complete_gesture(each_frame)\n",
    "        each_frame = each_frame[['Linear Acceleration x (m/s^2)', 'Linear Acceleration y (m/s^2)', \n",
    "                                 'Linear Acceleration z (m/s^2)', 'Absolute acceleration (m/s^2)']]\n",
    "        \n",
    "        features = each_frame.values.flatten()\n",
    "\n",
    "        knn_data_list.append(np.append(features, gesture_found))\n",
    "        frame_count += 1\n",
    "\n",
    "    # Use the length of the first item in knn_data_list to set column names\n",
    "    if knn_data_list:\n",
    "        column_names = [f'Feature {i}' for i in range(len(knn_data_list[0]) - 1)] + ['Gesture']\n",
    "        knn_data = pd.DataFrame(knn_data_list, columns=column_names)\n",
    "    else:\n",
    "        print_sys_msg(\"No data processed.\")\n",
    "\n",
    "    # Remove NaN values\n",
    "    knn_data.dropna(inplace=True)\n",
    "\n",
    "    # Data standardization and normalization\n",
    "    scaler = StandardScaler()\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    features = knn_data.columns[:-1]  # Exclude 'Gesture' column for scaling\n",
    "\n",
    "    knn_data[features] = scaler.fit_transform(knn_data[features])\n",
    "    knn_data[features] = min_max_scaler.fit_transform(knn_data[features])\n",
    "\n",
    "    # Balance the dataset\n",
    "    min_samples = knn_data['Gesture'].value_counts().min()\n",
    "    knn_data = knn_data.groupby('Gesture').apply(lambda x: x.sample(min_samples)).reset_index(drop=True)\n",
    "    print_sys_msg(f'min_samples: {min_samples}')\n",
    "\n",
    "    # Split the data\n",
    "    X = knn_data.iloc[:, :-1]\n",
    "    y = knn_data['Gesture']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Grid search for KNN hyperparameters\n",
    "    param_grid = {'n_neighbors': [3, 5, 7, 9, 11]}\n",
    "    grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_values = grid_search.best_params_\n",
    "    print_sys_msg('Best values:'+ str(best_values))\n",
    "\n",
    "    # Train and test KNN model\n",
    "    knn = KNeighborsClassifier(n_neighbors=best_values['n_neighbors'])\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy with cross-validation (5-fold)\n",
    "    scores = cross_val_score(knn, X, y, cv=5)\n",
    "    accuracy = scores.mean()\n",
    "    print_sys_msg('Accuracy:'+ str(accuracy))\n",
    "\n",
    "    # Store results in each object\n",
    "    knn_tried_neighbours.append(best_values['n_neighbors'])\n",
    "    knn_scores.append(scores)\n",
    "    knn_accuracy.append(accuracy)\n",
    "\n",
    "for i in range(3):\n",
    "    print_msg(f'KNN with step size {25 * (i + 1)}%')\n",
    "    print_msg(f'Tried neighbours: {knn_tried_neighbours[i]}')\n",
    "    print_msg(f'Scores: {knn_scores[i]}')\n",
    "    print_msg(f'Accuracy: {knn_accuracy[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = [25, 50, 75]\n",
    "print_sys_msg('Knn accuracy: '+str(knn_accuracy))\n",
    "print_sys_msg('Random Forest accuracy: '+str(random_forest_accuracy))\n",
    "print_sys_msg('SVM accuracy: '+str(svm_accuracy))\n",
    "\n",
    "knn_accuracy_percentage = [i * 100 for i in knn_accuracy]\n",
    "random_forest_accuracy_percentage = [i * 100 for i in random_forest_accuracy]\n",
    "svm_accuracy_percentage = [i * 100 for i in svm_accuracy]\n",
    "\n",
    "# visualizing the accuracy of the KNN, Random Forest, and SVM models with respect to the step size of sliding window in a bar chart\n",
    "fig, ax = plt.subplots()\n",
    "bar_width = 0.25\n",
    "bar1 = np.arange(len(step_size))\n",
    "bar2 = [i + bar_width for i in bar1]\n",
    "bar3 = [i + bar_width for i in bar2]\n",
    "\n",
    "plt.bar(bar1, knn_accuracy_percentage, color='b', width=bar_width, edgecolor='grey', label='KNN')\n",
    "plt.bar(bar2, random_forest_accuracy_percentage, color='r', width=bar_width, edgecolor='grey', label='Random Forest')\n",
    "plt.bar(bar3, svm_accuracy_percentage, color='g', width=bar_width, edgecolor='grey', label='SVM')\n",
    "\n",
    "plt.xlabel('Step Size (%)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy of KNN, Random Forest, and SVM with respect to Step Size of sliding window')\n",
    "plt.xticks([r + bar_width for r in range(len(step_size))], step_size)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
