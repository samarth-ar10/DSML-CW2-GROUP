{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TNT hand gestures study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flow of program\n",
    "DELETE BEFORE SUBMISSION\n",
    "\n",
    "-   code to import data from 3 csv files held in the data folder\n",
    "-   data is then combined in a dictionary and then converted to a pandas dataframe\n",
    "-   the data is then cleaned and the columns are renamed\n",
    "-   the data is then split into a training and testing set\n",
    "-   a linear regression model is then fitted to the training data\n",
    "-   the model is then used to predict the test data\n",
    "-   the mean squared error and r2 score are then calculated\n",
    "-   the results are then printed to the console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing relevant libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Topics covered till lab 7\n",
    "read Excel file: pd.read_excel()<br>\n",
    "read CSV file: pd.read_csv()<br>\n",
    "rename columns: df.rename()<br>\n",
    "unique values from a column: df.unique()<br>\n",
    "duplicated rows: df.duplicated()<br>\n",
    "drop duplicated rows: df.drop_duplicates()<br>\n",
    "quantile / percentile: df.quantile()<br>\n",
    "rows with NaN: df.isna()<br>\n",
    "drop rows with NaN: df.dropna()<br>\n",
    "determine the bin: pd.cut  \n",
    "assign numerical values to different categorical data: pd.get_dummies<br>\n",
    "determine data type: df.dtypes<br>\n",
    "plot regression plot: sns.regplot<br>\n",
    "calculate Pearson correlation: stats.pearsonr<br>\n",
    "\n",
    "In our previous labs, the main target was to find the most meaningful features for predicting the car price. In lab 7 we will try to develop different models to predict the car price using those meaningful features. The developed model will help us to understand how these variables are used to predict the result(car price) in our case.<br> The possible meaning features in car price prediction dataset are:<br>\n",
    "'''\n",
    "#-----------------Information-----------------#\n",
    "\n",
    "'''\n",
    "    Title: Linear Regression Model for Predicting Absolute Acceleration\n",
    "    Data Collection Declaration:\n",
    "\n",
    "    This project is being developed for a Data Science and Machine Learning class.\n",
    "    The data used in this project was collected by the student developers at the University of Nottingham. \n",
    "\n",
    "    Legal Aspects:\n",
    "\n",
    "    The data collection process complied with all applicable laws and university policies. \n",
    "    Any personal data that was collected has been anonymized to protect the privacy of the individuals involved. \n",
    "\n",
    "    Please note that the use of this data must comply with all relevant data protection and privacy laws. \n",
    "    Unauthorized use, disclosure, or duplication of this data is strictly prohibited.\n",
    "'''\n",
    "'''\n",
    "    Data Information:\n",
    "\n",
    "    Data within the dataset being examined is of the format of a csv file with the following columns:\n",
    "    Column Names and Types:\n",
    "    \n",
    "    '''\n",
    "\n",
    "#-----------------Information-----------------#\n",
    "\n",
    "#-----------------Importing Libraries-----------------#\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "# import k-nearest neighbors\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "\n",
    "#-----------------Importing Libraries-----------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "#-----------------Flags-----------------#\n",
    "\n",
    "SYS_MSG = True # Flag to toggle control over printing system messages to console \n",
    "PLOT = True # Flag to toggle control over plotting graphs\n",
    "TEST_PLOT = False # Flag to toggle control over plotting test graphs\n",
    "\n",
    "#-----------------Flags-----------------#\n",
    "\n",
    "#-----------------Basic Functions-----------------#\n",
    "\n",
    "# Function to print system messages\n",
    "def print_sys_msg(msg):\n",
    "    if SYS_MSG:\n",
    "        print('-'*10+'System control message'+'-'*10+'\\t\\t'+msg)\n",
    "\n",
    "# Function to print normal messages\n",
    "def print_msg(msg):\n",
    "    print('-'*10+'Message from program'+'-'*10+'\\t'+msg)\n",
    "\n",
    "#-----------------Basic Functions-----------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperation Phase  \n",
    "We create base classes to manage our data, visualisation, analysis, and prediction.  \n",
    "Creating DataHandler class. It wwill manage the import, stored manipulation, and final storage of data.  \n",
    "\\- [member name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#-----------------DataHandler Class-----------------#\n",
    "'''\n",
    "Class DataHandler\n",
    "    purpose: import and manage data (data manipulation, data wrangling, and data preprocessing)\n",
    "\n",
    "    initialization example:\n",
    "        data = DataHandler()\n",
    "    \n",
    "    functions:\n",
    "    - import_data: import data from csv files and combine them into a single dataframe\n",
    "        dependencies used: pandas\n",
    "        function call example: data.import_data(['data/member1.csv', 'data/member2.csv', 'data/member3.csv'])\n",
    "        input: list of file names\n",
    "        output: none\n",
    "    \n",
    "    - import_data_system: import data from the system\n",
    "        dependencies used: pandas, os\n",
    "        function call example: data.import_data_system(['data/member1', 'data/member2', 'data/member3'])\n",
    "        input: list of directory names\n",
    "        output: none\n",
    "        \n",
    "    - data_shape: print the shape of the dataframe\n",
    "        dependencies used: pandas\n",
    "        function call example: data.data_shape()\n",
    "        input: none\n",
    "        output: none\n",
    "\n",
    "    - data_head: print the first 5 rows of the dataframe\n",
    "        dependencies used: pandas\n",
    "        function call example: data.data_head()\n",
    "        input: none\n",
    "        output: none\n",
    "\n",
    "    - data_info: print the information of the dataframe\n",
    "        dependencies used: pandas\n",
    "        function call example: data.data_info()\n",
    "        input: none\n",
    "        output: none\n",
    "\n",
    "    - data_describe: print the description of the dataframe\n",
    "        dependencies used: pandas\n",
    "        function call example: data.data_describe()\n",
    "        input: none\n",
    "        output: none\n",
    "\n",
    "    - data_null: print the null values in the dataframe\n",
    "        dependencies used: pandas\n",
    "        function call example: data.data_null()\n",
    "        input: none\n",
    "        output: none\n",
    "    \n",
    "    - data_corr: print the correlation matrix of the dataframe\n",
    "        dependencies used: pandas\n",
    "        function call example: data.data_corr()\n",
    "        input: none\n",
    "        output: none\n",
    "\n",
    "    - drop_duplicates: drop duplicates        \n",
    "        dependencies used: pandas\n",
    "        function call example: data.drop_duplicates()\n",
    "        input: none\n",
    "        output: none\n",
    "\n",
    "    - drop_null: drop null values     \n",
    "        dependencies used: pandas\n",
    "        function call example: data.drop_null()\n",
    "        input: none\n",
    "        output: none\n",
    "    \n",
    "    - drop_outliers: drop outliers\n",
    "        dependencies used: pandas\n",
    "        function call example: data.drop_outliers()\n",
    "        input: none\n",
    "        output: none\n",
    "\n",
    "    - drop_negative_time: drop negative time values\n",
    "        dependencies used: pandas\n",
    "        function call example: data.drop_negative_time()\n",
    "        input: none\n",
    "        output: none\n",
    "        \n",
    "- Managed by: Samarth\n",
    "- Created on: 03/02/2024\n",
    "- Modified on: 03/02/2024\n",
    "- Contact:  psxs2@nottingham.ac.uk\n",
    "'''\n",
    "class DataHandler:\n",
    "    number_of_files = 0\n",
    "    data = None\n",
    "    df = None\n",
    "\n",
    "    def __init__(self):\n",
    "        print_sys_msg('DataHandler:__init__: DataHandler object created')\n",
    "    #-----------------Data Import Functions-----------------#\n",
    "    \n",
    "    # declaration example - data = DataHandler.import_data(['data/member1.csv', 'data/member2.csv', 'data/member3.csv'])\n",
    "    def import_data(self, files):\n",
    "            print_sys_msg('DataHandler:import_data: importing data from 3 csv files and combining them into a single dataframe')\n",
    "            number_of_files = len(files)\n",
    "            for i in range(number_of_files):\n",
    "                if i == 0:\n",
    "                    self.df = pd.read_csv(files[i])\n",
    "                    self.data = {'data_'+str(i+1): pd.read_csv(files[i])}\n",
    "                else:\n",
    "                    self.df = pd.concat([self.df, pd.read_csv(files[i])], ignore_index=True)\n",
    "                    self.data['data_'+str(i+1)] = pd.read_csv(files[i])\n",
    "            print_sys_msg('DataHandler:import_data: data imported successfully')\n",
    "\n",
    "    # declaration example - data = DataHandler.import_data_system(['data/member1/type_of_gesture/', 'data/member2/type_of_gesture/', 'data/member3/type_of_gesture/'])\n",
    "    def import_data_system(self, directorys):\n",
    "        print_sys_msg('DataHandler:import_data: importing data from the system')\n",
    "        \n",
    "        # all files are stored in the format data/member1/type_of_gesture/gesture1/Raw Data.csv\n",
    "\n",
    "        # getting the number of members\n",
    "        number_of_members = len(directorys)\n",
    "\n",
    "        print_sys_msg('DataHandler:import_data: number of members: '+str(number_of_members))\n",
    "\n",
    "        for i in range(number_of_members):\n",
    "            # member name from the directory\n",
    "            member = directorys[i].split('/')[1]\n",
    "            # gesture name from the directory\n",
    "            gesture = directorys[i].split('/')[2]\n",
    "            # getting the number of gestures\n",
    "            number_of_gestures = len(os.listdir(directorys[i]))\n",
    "\n",
    "            print_sys_msg('DataHandler:import_data: member: '+member)\n",
    "            print_sys_msg('DataHandler:import_data: gesture name: '+gesture)\n",
    "            print_sys_msg('DataHandler:import_data: number of gestures: '+str(number_of_gestures))\n",
    "            \n",
    "            for j in range(number_of_gestures):\n",
    "                # reading name of each gesture and then grabbing the 'Raw Data.csv' file from it\n",
    "                gesture_name = os.listdir(directorys[i])[j]\n",
    "                \n",
    "                # reading the 'Raw Data.csv' file inside the gesture folder\n",
    "                # if self.data is None:\n",
    "                #     self.data = {'data_'+member+'_'+gesture+'_'+'_'+str(j+1): pd.read_csv(directorys[i]+gesture_name+'/Raw Data.csv')}\n",
    "                if self.data is None:\n",
    "                    self.data = {'data_'+member+'_'+gesture+'_'+gesture_name: pd.read_csv(directorys[i]+gesture_name+'/Raw Data.csv')}                \n",
    "                # else: \n",
    "                #   self.data['data_'+member+'_'+gesture+'_'+str(j+1)] = pd.read_csv(directorys[i]+gesture_name+'/Raw Data.csv')\n",
    "                else:\n",
    "                    self.data['data_'+member+'_'+gesture+'_'+gesture_name] = pd.read_csv(directorys[i]+gesture_name+'/Raw Data.csv')\n",
    "\n",
    "                # # adding gesture_name to the dictionary of data\n",
    "                # self.data['data_'+member+'_'+gesture+'_'+str(j+1)]['Gesture'] = gesture_name\n",
    "        \n",
    "        print_sys_msg('DataHandler:import_data: data imported successfully')\n",
    "        # print the dictionary names \n",
    "        print_sys_msg('DataHandler:import_data: printing the dictionary names')\n",
    "        print_sys_msg(str(self.data.keys()))\n",
    "        \n",
    "\n",
    "        \n",
    "    #-----------------Data Import Functions-----------------#\n",
    "                    \n",
    "    \n",
    "    #-----------------Basic Data Wrangling Functions-----------------#\n",
    "    \n",
    "    def data_shape(self):\n",
    "        print_sys_msg('DataHandler:data_shape: printing the shape of the dataframe')\n",
    "        print_sys_msg(str(self.df.shape))\n",
    "    def data_head(self):\n",
    "        print_sys_msg('DataHandler:data_head: printing the first 5 rows of the dataframe')\n",
    "        print_sys_msg(str(self.df.head()))\n",
    "    \n",
    "    def data_info(self):\n",
    "        print_sys_msg('DataHandler:data_info: printing the information of the dataframe')\n",
    "        print_sys_msg(str(self.df.info()))\n",
    "                      \n",
    "    def data_describe(self):\n",
    "        print_sys_msg('DataHandler:data_describe: printing the description of the dataframe')\n",
    "        print_sys_msg(str(self.df.describe()))\n",
    "                      \n",
    "    def data_null(self):\n",
    "        print_sys_msg('DataHandler:data_null: printing the null values in the dataframe')\n",
    "        print_sys_msg(str(self.df.isnull().sum()))\n",
    "                      \n",
    "    def data_corr(self):\n",
    "        print_sys_msg('DataHandler:data_corr: printing the correlation matrix of the dataframe')\n",
    "        print_sys_msg(str(self.df.corr()))\n",
    "    \n",
    "    def data_missing(self):\n",
    "        print_sys_msg('DataHandler:data_missing: printing the missing values in the dataframe')\n",
    "        print_sys_msg(str(self.df.isna().any(axis=1)))\n",
    "    #-----------------Basic Data Wrangling Functions-----------------#\n",
    "        \n",
    "    #-----------------Data Preprocessing Functions-----------------#\n",
    "\n",
    "    # add_to_data: add data to the dataframe\n",
    "    def add_to_data(self, data):\n",
    "        print_sys_msg('DataHandler:add_to_data: adding data to the dataframe')\n",
    "        self.df = pd.concat([self.df, data], ignore_index=True)\n",
    "\n",
    "    def drop_duplicates(self):\n",
    "        print_sys_msg('DataHandler:drop_duplicates: dropping duplicates')\n",
    "        self.df = self.df.drop_duplicates()\n",
    "    \n",
    "    def drop_null(self):\n",
    "        print_sys_msg('DataHandler:drop_null: dropping null values')\n",
    "        self.df = self.df.dropna()\n",
    "\n",
    "\n",
    "    # def drop_outliers(self):\n",
    "    #     print_sys_msg('DataHandler:drop_outliers: dropping outliers'\n",
    "    #     self.df = self.df[(self.df['Linear Acceleration x (m/s^2)'] > -10) & (self.df['Linear Acceleration x (m/s^2)'] < 10)]\n",
    "    #     self.df = self.df[(self.df['Linear Acceleration y (m/s^2)'] > -10) & (self.df['Linear Acceleration y (m/s^2)'] < 10)]\n",
    "    #     self.df = self.df[(self.df['Linear Acceleration z (m/s^2)'] > -10) & (self.df['Linear Acceleration z (m/s^2)'] < 10)]\n",
    "    #     self.df = self.df[(self.df['Absolute acceleration (m/s^2)'] > 0) & (self.df['Absolute acceleration (m/s^2)'] < 10)]\n",
    "\n",
    "    def drop_negative_time(self):\n",
    "        print_sys_msg('DataHandler:drop_negative_time: dropping negative time values')\n",
    "        self.df = self.df[self.df['Time (s)'] > 0]\n",
    "\n",
    "    # missing values handling - drop rows with missing values\n",
    "    def drop_missing(self, threshold=3):\n",
    "        print_sys_msg('DataHandler:drop_missing: dropping missing values')\n",
    "        self.df = self.df.dropna(thresh=threshold).copy()\n",
    "\n",
    "    # missing values handling - fill missing values with mean of the column\n",
    "    def fill_missing(self):\n",
    "        print_sys_msg('DataHandler:fill_missing: filling missing values with mean of the column')\n",
    "        self.df = self.df.fillna(self.df.mean())\n",
    "\n",
    "    # missing values handling - fill missing values with median of the column\n",
    "    def fill_missing_median(self):\n",
    "        print_sys_msg('DataHandler:fill_missing_median: filling missing values with median of the column')\n",
    "        self.df = self.df.fillna(self.df.median())\n",
    "    \n",
    "    # missing values handling - fill missing values with mode of the column\n",
    "    def fill_missing_mode(self):\n",
    "        print_sys_msg('DataHandler:fill_missing_mode: filling missing values with mode of the column')\n",
    "        self.df = self.df.fillna(self.df.mode().iloc[0])\n",
    "    \n",
    "    # missing values handling - fill missing values with bill debth of the column\n",
    "    #-----------------------------------\n",
    "    #-----------------------------------To be written\n",
    "    #-----------------------------------\n",
    "    \n",
    "    # data normalization - min-max normalization\n",
    "    def min_max_normalization(self):\n",
    "        print_sys_msg('DataHandler:min_max_normalization: min-max normalization')\n",
    "        self.df = (self.df - self.df.min()) / (self.df.max() - self.df.min())\n",
    "    \n",
    "    # data normalization - standardization\n",
    "    def standardization(self):\n",
    "        print_sys_msg('DataHandler:standardization: standardization')\n",
    "        self.df = (self.df - self.df.mean()) / self.df.std()\n",
    "    \n",
    "\n",
    "    #-----------------Data Preprocessing Functions-----------------#\n",
    "\n",
    "\n",
    "    #-----------------Data Splitting Functions-----------------#\n",
    "    \n",
    "    #-----------------Data Splitting Functions-----------------#\n",
    "        \n",
    "    #-----------------Storing Data Functions-----------------#\n",
    "\n",
    "    def store_data_with_name(self, file_name):\n",
    "        print_sys_msg('DataHandler:store_data: storing data to a csv file')\n",
    "        self.df.to_csv(file_name, index=False)\n",
    "    \n",
    "    def store_data_with_current_date_time(self):\n",
    "        print_sys_msg('DataHandler:store_data_with_current_date_time: storing data to a csv file with current date and time')\n",
    "        self.df.to_csv('data_'+str(pd.to_datetime('today'))+'.csv', index=False)\n",
    "    \n",
    "    def store_data_with_index(self):\n",
    "        print_sys_msg('DataHandler:store_data_with_index: storing data to a csv file with index')\n",
    "        \n",
    "        # data is stored with index only\n",
    "        # getting the highest index of the data in the data folder and then incrementing it by 1\n",
    "        # storing the data with the new index\n",
    "        #-----------------------------------\n",
    "        #-----------------------------------To be written\n",
    "\n",
    "    #-----------------Storing Data Functions-----------------#\n",
    "\n",
    "#-----------------DataHandler Class-----------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we make a basic visualization class that will manage the plotting and core visualization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#-----------------DataVisualization Class-----------------#\n",
    "'''\n",
    "Class DataVisualization\n",
    "\n",
    "    purpose: visualize data (data visualization)\n",
    "    charts included: line, scatter, bar, histogram, box plot, violin plot, bullet, table, sparkline, connected scatter plot, box, pie, doughnut, gauge, waffle\n",
    "    \n",
    "    functions:\n",
    "    -  __init__: initialize the object\n",
    "        dependencies used: none\n",
    "        function call example: data_visualization = DataVisualization()\n",
    "        input: none\n",
    "        output: none\n",
    "\n",
    "    - switch_to_seaborn: switch to seaborn\n",
    "        dependencies used: none\n",
    "        function call example: data_visualization.switch_to_seaborn(True)\n",
    "        input: boolean flag\n",
    "        output: none\n",
    "\n",
    "    - add_index: add index\n",
    "        dependencies used: none\n",
    "        function call example: data_visualization.add_index(index)\n",
    "        input: index\n",
    "        output: none\n",
    "\n",
    "    - add_data: add data\n",
    "        dependencies used: none\n",
    "        function call example: data_visualization.add_data(data)\n",
    "        input: data\n",
    "        output: none\n",
    "\n",
    "    - plot_chart: plot chart\n",
    "        dependencies used: matplotlib, seaborn\n",
    "        function call example: data_visualization.plot_chart('line', 'cyan', 0.8)\n",
    "        input: type, color, thickness\n",
    "        output: none\n",
    "\n",
    "    - set_grid_params: set grid parameters\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.set_grid_params(2, 2, 20, 5, 'Title')\n",
    "        input: rows, cols, figsize_x, figsize_y, title\n",
    "        output: none\n",
    "\n",
    "    - plot_grid_1d: plot grid 1d\n",
    "        dependencies used: matplotlib, seaborn\n",
    "        function call example: data_visualization.plot_grid_1d(0, 'line', 'x', 'y', 'Title', 'cyan', 0.8)\n",
    "        input: count, type, name_x, name_y, plot_title, color, thickness    \n",
    "        output: none\n",
    "\n",
    "    - plot_grid_2d: plot grid 2d\n",
    "        dependencies used: matplotlib, seaborn\n",
    "        function call example: data_visualization.plot_grid_2d(0, 0, 'line', 'x', 'y', 'Title', 'cyan', 0.8)\n",
    "        input: row, col, type, name_x, name_y, plot_title, color, thickness\n",
    "        output: none\n",
    "\n",
    "    - set_x_label: set x label\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.set_x_label('x')\n",
    "        input: label\n",
    "        output: none\n",
    "\n",
    "    - set_y_label: set y label\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.set_y_label('y')\n",
    "        input: label\n",
    "        output: none\n",
    "\n",
    "    - set_x_tick_labels: set x tick labels\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.set_x_tick_labels(labels)\n",
    "        input: labels\n",
    "        output: none\n",
    "\n",
    "    - set_y_tick_labels: set y tick labels\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.set_y_tick_labels(labels)\n",
    "        input: labels\n",
    "        output: none\n",
    "\n",
    "    - figure_size: set figure size\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.figure_size(20, 5)\n",
    "        input: width, height\n",
    "        output: none\n",
    "\n",
    "    - set_title: set title\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.set_title('Title')\n",
    "        input: title\n",
    "        output: none\n",
    "\n",
    "    - clear_plot: clear plot\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.clear_plot()\n",
    "        input: none\n",
    "        output: none\n",
    "\n",
    "    - show_plot: show plot\n",
    "        dependencies used: matplotlib\n",
    "        function call example: data_visualization.show_plot()\n",
    "        input: none\n",
    "        output: none\n",
    "   \n",
    "- Managed by: \n",
    "- Created on: 03/02/2024\n",
    "- Modified on: 03/02/2024\n",
    "- Contact: @nottingham.ac.uk\n",
    "'''\n",
    "class DataVisualization:\n",
    "    \n",
    "    # boolean flag to determine if the plot is matplotlib or seaborn\n",
    "    is_seaborn = False\n",
    "\n",
    "    def __init__(self, index):\n",
    "        self.add_index(index)\n",
    "        self.data = []\n",
    "\n",
    "    def switch_to_seaborn(self, flag):\n",
    "        self.is_seaborn = flag\n",
    "\n",
    "    def add_index(self, index):\n",
    "        self.index = index.copy()\n",
    "\n",
    "    def add_data(self, data):\n",
    "        self.data = data.copy()\n",
    "\n",
    "    # def plot_chart(self, type, c='cyan', thickness=0.8, legend=None):\n",
    "    def plot_chart(self, type, name_x, name_y, plot_title, c='cyan', thickness=0.8, legend=None):\n",
    "        # charts included: line, scatter, bar, histogram, box plot, violin plot, bullet, table, sparkline, connected scatter plot, box, pie, doughnut, gauge, waffle\n",
    "        if type == 'line':\n",
    "            sns.lineplot(x=self.index, y=self.data, c=c, linewidth=thickness) if self.is_seaborn else plt.plot(self.index, self.data, c=c, linewidth=thickness)\n",
    "        elif type == 'scatter':\n",
    "            sns.scatterplot(x=self.index, y=self.data, c=c) if self.is_seaborn else plt.scatter(self.index, self.data, c=c)\n",
    "        elif type == 'bar':\n",
    "            sns.barplot(x=self.index, y=self.data, c=c) if self.is_seaborn else plt.bar(self.index, self.data, c=c)\n",
    "        elif type == 'histogram':\n",
    "            sns.histplot(self.data, c=c) if self.is_seaborn else plt.hist(self.data, c=c)\n",
    "        elif type == 'box plot':\n",
    "            sns.boxplot(self.data, c=c) if self.is_seaborn else plt.boxplot(self.data, c=c)\n",
    "        elif type == 'violin plot':\n",
    "            sns.violinplot(self.data, c=c) if self.is_seaborn else plt.violinplot(self.data, c=c)\n",
    "        elif type == 'bullet':\n",
    "            sns.bullet(self.data, c=c) if self.is_seaborn else plt.bullet(self.data, c=c)\n",
    "        elif type == 'table':\n",
    "            sns.table(self.data, c=c) if self.is_seaborn else plt.table(self.data, c=c)\n",
    "        elif type == 'sparkline':\n",
    "            sns.sparkline(self.data, c=c) if self.is_seaborn else plt.sparkline(self.data, c=c)\n",
    "        elif type == 'connected scatter plot':\n",
    "            sns.lineplot(x=self.index, y=self.data, sort=False, c=c) if self.is_seaborn else plt.plot(self.index, self.data, c=c)\n",
    "        elif type == 'box':\n",
    "            sns.boxplot(self.data, c=c) if self.is_seaborn else plt.boxplot(self.data, c=c)\n",
    "        elif type == 'pie':\n",
    "            plt.pie(self.data, labels=self.index)\n",
    "        elif type == 'doughnut':\n",
    "            plt.pie(self.data, labels=self.index, wedgeprops=dict(width=0.5))\n",
    "        elif type == 'gauge':\n",
    "            plt.pie(self.data, labels=self.index, wedgeprops=dict(width=0.2))\n",
    "        elif type == 'waffle':\n",
    "            plt.pie(self.data, labels=self.index, wedgeprops=dict(width=0.1))\n",
    "        else:\n",
    "            print('Invalid chart type')\n",
    "        \n",
    "        if legend is not None:\n",
    "            plt.legend(legend, loc='upper right')\n",
    "    \n",
    "    def set_grid_params(self, rows, cols, figsize_x, figsize_y, title):\n",
    "        if rows <= 0 or cols <= 0:\n",
    "            print_sys_msg('DataVisualization:set_grid_params: rows and cols should be greater than 0')\n",
    "            return\n",
    "        self.fig, self.ax = plt.subplots(rows, cols, figsize=(figsize_x, figsize_y))\n",
    "        self.fig.suptitle(title)\n",
    "    \n",
    "    def plot_grid_1d(self, count, type, name_x, name_y, plot_title, c= 'cyan', thickness=0.8, legend=None):\n",
    "        \n",
    "        if count <= 1:\n",
    "            self.plot_chart(type, color, thickness)\n",
    "            return\n",
    "        self.ax[count].set_xlabel(name_x)\n",
    "        self.ax[count].set_ylabel(name_y)\n",
    "        self.ax[count].set_title(plot_title)\n",
    "        if type == 'line':\n",
    "            sns.lineplot(x=self.index, y=self.data, c=c, linewidth=thickness, ax=self.ax[count]) if self.is_seaborn else self.ax[count].plot(self.index, self.data, c=c, linewidth=thickness)\n",
    "        elif type == 'scatter':\n",
    "            sns.scatterplot(x=self.index, y=self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].scatter(self.index, self.data, c=c)\n",
    "        elif type == 'bar':\n",
    "            sns.barplot(x=self.index, y=self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].bar(self.index, self.data, c=c)\n",
    "        elif type == 'histogram':\n",
    "            sns.histplot(self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].hist(self.data, c=c)\n",
    "        elif type == 'box plot':\n",
    "            sns.boxplot(self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].boxplot(self.data, c=c)\n",
    "        elif type == 'violin plot':\n",
    "            sns.violinplot(self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].violinplot(self.data, c=c)\n",
    "        elif type == 'bullet':\n",
    "            sns.bullet(self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].bullet(self.data, c=c)\n",
    "        elif type == 'table':\n",
    "            sns.table(self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].table(self.data, c=c)\n",
    "        elif type == 'sparkline':\n",
    "            sns.sparkline(self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].sparkline(self.data, c=c)\n",
    "        elif type == 'connected scatter plot':\n",
    "            sns.lineplot(x=self.index, y=self.data, sort=False, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].plot(self.index, self.data, c=c)\n",
    "        elif type == 'box':\n",
    "            sns.boxplot(self.data, c=c, ax=self.ax[count]) if self.is_seaborn else self.ax[count].boxplot(self.data, c=c)\n",
    "        elif type == 'pie':\n",
    "            self.ax[count].pie(self.data, labels=self.index)\n",
    "        elif type == 'doughnut':\n",
    "            self.ax[count].pie(self.data, labels=self.index, wedgeprops=dict(width=0.5))\n",
    "        elif type == 'gauge':\n",
    "            self.ax[count].pie(self.data, labels=self.index, wedgeprops=dict(width=0.2))\n",
    "        elif type == 'waffle':\n",
    "            self.ax[count].pie(self.data, labels=self.index, wedgeprops=dict(width=0.1))\n",
    "        else:\n",
    "            print('Invalid chart type')\n",
    "\n",
    "        if legend is not None:\n",
    "            self.ax[count].legend(legend, loc='upper right')\n",
    "\n",
    "    def plot_grid_2d(self, row, col, type, name_x, name_y, plot_title, c='cyan', thickness=0.8, legend=None):\n",
    "        self.ax[row, col].set_xlabel(name_x)\n",
    "        self.ax[row, col].set_ylabel(name_y)\n",
    "        self.ax[row, col].set_title(plot_title)\n",
    "\n",
    "        if type == 'line':\n",
    "            sns.lineplot(x=self.index, y=self.data, c=c, linewidth=thickness, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].plot(self.index, self.data, c=c, linewidth=thickness)\n",
    "        elif type == 'scatter':\n",
    "            sns.scatterplot(x=self.index, y=self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].scatter(self.index, self.data, c=c)\n",
    "        elif type == 'bar':\n",
    "            sns.barplot(x=self.index, y=self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].bar(self.index, self.data, c=c)\n",
    "        elif type == 'histogram':\n",
    "            sns.histplot(self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].hist(self.data, c=c)\n",
    "        elif type == 'box plot':\n",
    "            sns.boxplot(self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].boxplot(self.data, c=c)\n",
    "        elif type == 'violin plot':\n",
    "            sns.violinplot(self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].violinplot(self.data, c=c)\n",
    "        elif type == 'bullet':\n",
    "            sns.bullet(self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].bullet(self.data, c=c)\n",
    "        elif type == 'table':\n",
    "            sns.table(self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].table(self.data, c=c)\n",
    "        elif type == 'sparkline':\n",
    "            sns.sparkline(self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].sparkline(self.data, c=c)\n",
    "        elif type == 'connected scatter plot':\n",
    "            sns.lineplot(x=self.index, y=self.data, sort=False, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].plot(self.index, self.data, c=c)\n",
    "        elif type == 'box':\n",
    "            sns.boxplot(self.data, c=c, ax=self.ax[row, col]) if self.is_seaborn else self.ax[row, col].boxplot(self.data, c=c)\n",
    "        elif type == 'pie':\n",
    "            self.ax[row, col].pie(self.data, labels=self.index)\n",
    "        elif type == 'doughnut':\n",
    "            self.ax[row, col].pie(self.data, labels=self.index, wedgeprops=dict(width=0.5))\n",
    "        elif type == 'gauge':\n",
    "            self.ax[row, col].pie(self.data, labels=self.index, wedgeprops=dict(width=0.2))\n",
    "        elif type == 'waffle':\n",
    "            self.ax[row, col].pie(self.data, labels=self.index, wedgeprops=dict(width=0.1))\n",
    "        else:\n",
    "            print('Invalid chart type')\n",
    "\n",
    "        if legend is not None:\n",
    "            self.ax[row, col].legend(legend, loc='upper right')\n",
    "\n",
    "    def set_x_label(self, label):\n",
    "        plt.xlabel(label)\n",
    "\n",
    "    def set_y_label(self, label):\n",
    "        plt.ylabel(label)\n",
    "\n",
    "    def set_x_tick_labels(self, labels):\n",
    "        plt.xticks(self.index, labels)\n",
    "    \n",
    "    def set_y_tick_labels(self, labels):\n",
    "        plt.yticks(self.index, labels)\n",
    "    \n",
    "    def figure_size(self, width = 20, height = 5):\n",
    "        plt.figure(figsize=(width, height))\n",
    "    \n",
    "    def set_title(self, title):\n",
    "        plt.title(title)\n",
    "\n",
    "    def clear_plot(self):\n",
    "        plt.clf()\n",
    "        \n",
    "    def show_plot(self):\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# class sliding time window. it will create frames of dataframes based on the sliding time window size. \n",
    "# window_size -> size of the window which represents the number of rows in the dataframe\n",
    "# step_size -> size of the step which represents the number of rows to move the window by \n",
    "# data -> dataframe which will be used to create frames\n",
    "# create_frames -> function to create frames of dataframes based on the sliding time window size\n",
    "# get_frames -> function to get the frames\n",
    "\n",
    "\n",
    "class SlidingTimeWindow:\n",
    "    def __init__(self, data, window_size, step_size):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        self.frames = []\n",
    "    \n",
    "    def create_frames(self):\n",
    "        for i in range(0, len(self.data), self.step_size):\n",
    "            if i + self.window_size < len(self.data):\n",
    "                self.frames.append(self.data.iloc[i:i+self.window_size])\n",
    "            else:\n",
    "                self.frames.append(self.data.iloc[i:])\n",
    "        return self.frames\n",
    "    \n",
    "    def get_frames(self):\n",
    "        return self.frames\n",
    "\n",
    "\n",
    "# class DBSCANClustering. it will two groups using DBSCAN clustering algorithm for each column in the dataframe except the time, and gesture column. Also it will have a function to visualize it\n",
    "\n",
    "class DBSCANClustering:\n",
    "    def __init__(self, data, eps, min_samples):\n",
    "        self.data = data\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "        self.clusters = []\n",
    "    \n",
    "    def cluster(self):\n",
    "        for column in self.data.columns:\n",
    "            print_sys_msg('DBSCANClustering:cluster: column: '+column)\n",
    "            if column != 'Time (s)' and column != 'Gesture':\n",
    "                print_sys_msg('DBSCANClustering:cluster: clustering started for column: '+column)  \n",
    "                clustering = DBSCAN(eps=self.eps, min_samples=self.min_samples).fit(self.data[column].values.reshape(-1, 1))\n",
    "                self.clusters.append(clustering.labels_)\n",
    "        print_sys_msg('DBSCANClustering:cluster: clustering done')\n",
    "        print_sys_msg('DBSCANClustering:cluster: clusters: '+str(len(self.clusters)))\n",
    "        return self.clusters\n",
    "    \n",
    "    def visualize_with_DataVisualization(self, include_original_data = False, sns = False):\n",
    "        for i in range(len(self.clusters)):\n",
    "            if self.data.columns[i] != 'Time (s)':\n",
    "                dv = DataVisualization(self.data['Time (s)'])\n",
    "                if sns == True:\n",
    "                    dv.switch_to_seaborn(True)\n",
    "                dv.add_data(self.data.iloc[:, i])\n",
    "                dv.plot_chart('scatter', 'cyan', 0.8)\n",
    "                if include_original_data == True:\n",
    "                    dv.plot_chart('line', 'red', 0.3)\n",
    "                dv.set_x_label('Time (s)')\n",
    "                dv.set_y_label(self.data.columns[i])\n",
    "                dv.set_title(self.data.columns[i])\n",
    "                dv.show_plot()\n",
    "                if sns == True:\n",
    "                    dv.switch_to_seaborn(False)\n",
    "    \n",
    "    def visualize_with_DataVisualization_selected_column(self, column, include_original_data = False, sns = False): # column is the index of the column. 1- linear acceleration x, 2- linear acceleration y, 3- linear acceleration z, 4- absolute acceleration\n",
    "        dv = DataVisualization(self.data['Time (s)'])\n",
    "        if sns == True:\n",
    "            dv.switch_to_seaborn(True)\n",
    "        dv.add_data(self.data.iloc[:, column])\n",
    "        dv.plot_chart('scatter', c=self.clusters[column], thickness=0.8)\n",
    "        if include_original_data == True:\n",
    "            dv.plot_chart('line', c='red', thickness=0.3)\n",
    "        dv.set_x_label('Time (s)')\n",
    "        dv.set_y_label(self.data.columns[column])\n",
    "        dv.set_title(self.data.columns[column])\n",
    "        dv.show_plot()\n",
    "        if sns == True:\n",
    "            dv.switch_to_seaborn(False)\n",
    "        \n",
    "    def visualize_with_DataVisualization_2d(self, topic = 'DBSCAN Clustering', include_original_data = False, sns = False, save=False, file_path_name='DBSCAN Clustering', visualize_in_terminal=False):\n",
    "        # use dv.plot_grid_2d\n",
    "        dv = DataVisualization(self.data['Time (s)'])\n",
    "        if sns == True:\n",
    "            dv.switch_to_seaborn(True)\n",
    "\n",
    "        dv.set_grid_params(2, 2, 20, 10, topic)\n",
    "\n",
    "        dv.add_data(self.data.iloc[:, 1])\n",
    "        dv.plot_grid_2d(0, 0, 'scatter', 'Time (s)', self.data.columns[1], self.data.columns[1], c=self.clusters[0])\n",
    "        if include_original_data == True:\n",
    "            dv.plot_grid_2d(0, 0, 'line' , 'Time (s)', self.data.columns[1], self.data.columns[1], c='red', thickness=0.3)\n",
    "\n",
    "        dv.add_data(self.data.iloc[:, 2])\n",
    "        dv.plot_grid_2d(0, 1, 'scatter', 'Time (s)', self.data.columns[2], self.data.columns[2], c=self.clusters[1])\n",
    "        if include_original_data == True:\n",
    "            dv.plot_grid_2d(0, 1, 'line' , 'Time (s)', self.data.columns[2], self.data.columns[2], c='red', thickness=0.3)\n",
    "\n",
    "        dv.add_data(self.data.iloc[:, 3])\n",
    "        dv.plot_grid_2d(1, 0, 'scatter', 'Time (s)', self.data.columns[3], self.data.columns[3], c=self.clusters[2])\n",
    "        if include_original_data == True:\n",
    "            dv.plot_grid_2d(1, 0, 'line' , 'Time (s)', self.data.columns[3], self.data.columns[3], c='red', thickness=0.3)\n",
    "            \n",
    "        dv.add_data(self.data.iloc[:, 4])\n",
    "        dv.plot_grid_2d(1, 1, 'scatter', 'Time (s)', self.data.columns[4], self.data.columns[4], c=self.clusters[3])\n",
    "        if include_original_data == True:\n",
    "            dv.plot_grid_2d(1, 1, 'line' , 'Time (s)', self.data.columns[4], self.data.columns[4], c='red', thickness=0.3)\n",
    "        \n",
    "        if visualize_in_terminal == True:\n",
    "            dv.show_plot()\n",
    "        if sns == True:\n",
    "            dv.switch_to_seaborn(False)\n",
    "        if save == True:\n",
    "            file_path_name = 'cluster_data/'+file_path_name\n",
    "            dv.fig.savefig(file_path_name)\n",
    "\n",
    "    def Visualize_best_esp_min_samples(self, best_values_df, eps, min_samples, include_original_data = False, sns = False, save=False, file_path_name='DBSCAN Clustering'):\n",
    "        # create a heatmap with index as eps and columns as min_samples and values as the number of unique clusters found in linear acceleration x, linear acceleration y, linear acceleration z, and absolute acceleration for each key in best_values_df.\n",
    "        self.best_values_df = best_values_df\n",
    "        dv = DataVisualization(self.best_values_df['eps'])\n",
    "        keys = self.best_values_df['key'].unique()\n",
    "        for key in keys:\n",
    "            # create a heatmap for each key in best_values_df with index as eps and columns as min_samples and values as the number of unique clusters found in linear acceleration x, linear acceleration y, linear acceleration z, and absolute acceleration. make 4 seperate heatmaps for each acceleration in a grid of 2 x 2. use sns.heatmap to create the heatmap\n",
    "            dv.switch_to_seaborn(True)\n",
    "            dv.set_grid_params(2, 2, 20, 15, key)\n",
    "            sns.heatmap(self.best_values_df[self.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_linear_acceleration_x'), ax=dv.ax[0, 0], cmap='coolwarm')\n",
    "            dv.ax[0, 0].set_title('unique_clusters_linear_acceleration_x')\n",
    "            sns.heatmap(self.best_values_df[self.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_linear_acceleration_y'), ax=dv.ax[0, 1], cmap='coolwarm')\n",
    "            dv.ax[0, 1].set_title('unique_clusters_linear_acceleration_y')\n",
    "            sns.heatmap(self.best_values_df[self.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_linear_acceleration_z'), ax=dv.ax[1, 0], cmap='coolwarm')\n",
    "            dv.ax[1, 0].set_title('unique_clusters_linear_acceleration_z')\n",
    "            sns.heatmap(self.best_values_df[self.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_absolute_acceleration'), ax=dv.ax[1, 1], cmap='coolwarm')\n",
    "            dv.ax[1, 1].set_title('unique_clusters_absolute_acceleration')\n",
    "            dv.show_plot()\n",
    "            dv.switch_to_seaborn(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA Initiation Phase  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import  \n",
    "importing 3 seperate files containing the data from phyphox of each member of the team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_circles = DataHandler()\n",
    "data_circles.import_data_system(['data/anakha/circles/'])\n",
    "data_circles.name='circles'\n",
    "\n",
    "data_come_here = DataHandler()\n",
    "data_come_here.import_data_system(['data/anakha/come here/'])\n",
    "data_come_here.name='come here'\n",
    "\n",
    "data_go_away = DataHandler()\n",
    "data_go_away.import_data_system(['data/anakha/go away/'])\n",
    "data_go_away.name='go away'\n",
    "\n",
    "data_wave = DataHandler()\n",
    "data_wave.import_data_system(['data/anakha/wave/'])\n",
    "data_wave.name='wave'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if False: # set to True to visualize the raw data\n",
    "    # show linear accelaration x, y, z in a plot alongside Absolute acceleration to the right in a grid for circle the keys in the dictionary data.data\n",
    "    for gesture in [data_circles, data_come_here, data_go_away, data_wave]:\n",
    "        for key in gesture.data.keys():\n",
    "            print_sys_msg(key) \n",
    "            dv = DataVisualization(gesture.data[key]['Time (s)'])\n",
    "\n",
    "            dv.set_grid_params(2, 2, 20, 15, gesture.name+' -> '+key)\n",
    "            dv.add_data(gesture.data[key]['Linear Acceleration x (m/s^2)'])\n",
    "            dv.plot_grid_2d(0, 0, 'line', 'Time (s)', 'Linear Acceleration x (m/s^2)', 'Linear Acceleration x (m/s^2) vs Time (s)', \"red\")\n",
    "            dv.add_data(gesture.data[key]['Linear Acceleration y (m/s^2)'])\n",
    "            dv.plot_grid_2d(0, 1, 'line', 'Time (s)', 'Linear Acceleration y (m/s^2)', 'Linear Acceleration y (m/s^2) vs Time (s)', \"green\")\n",
    "            dv.add_data(gesture.data[key]['Linear Acceleration z (m/s^2)'])\n",
    "            dv.plot_grid_2d(1, 0, 'line', 'Time (s)', 'Linear Acceleration z (m/s^2)', 'Linear Acceleration z (m/s^2) vs Time (s)', \"blue\")\n",
    "            dv.add_data(gesture.data[key]['Absolute acceleration (m/s^2)'])\n",
    "            dv.plot_grid_2d(1, 1, 'line', 'Time (s)', 'Absolute acceleration (m/s^2)', 'Absolute acceleration (m/s^2) vs Time (s)')\n",
    "\n",
    "            dv.show_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for gesture in [data_circles, data_come_here, data_go_away, data_wave]:\n",
    "# for gesture in [data_wave]:\n",
    "    print_sys_msg(\"starting pre-labelling analysis for gesture: \"+gesture.name)\n",
    "\n",
    "    best_values = []  # List to store the data of the best values of eps and min_samples\n",
    "    eps = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    min_samples = [20, 30, 40, 50, 100, 200]\n",
    "    \n",
    "    for key in gesture.data.keys():\n",
    "        print_sys_msg(key)\n",
    "        # print_msg(\"printing sample data with eps: 0.3 and min_samples: 30\")\n",
    "        # dbscan = DBSCANClustering([gesture].data[key], 0.3, 30)\n",
    "        # dbscan.cluster()\n",
    "        # dbscan.visualize_with_DataVisualization_2d(key, include_original_data=True, sns=True)\n",
    "        \n",
    "        for e in eps:\n",
    "            for m in min_samples:\n",
    "                print_msg(key+' -> eps: '+str(e)+' min_samples: '+str(m))\n",
    "    \n",
    "                dbscan = DBSCANClustering(gesture.data[key], e, m)\n",
    "                dbscan.cluster()\n",
    "                print_msg('unique clusters found in linear acceleration x: '+str(np.unique(dbscan.clusters[0]))+' total clusters: '+str(len(np.unique(dbscan.clusters[0]))))\n",
    "                print_msg('unique clusters found in linear acceleration x has the following cluster density: ')\n",
    "                for cluster in np.unique(dbscan.clusters[0]):\n",
    "                    print_msg('cluster: '+str(cluster)+' density: '+str(len(dbscan.clusters[0][dbscan.clusters[0] == cluster])))\n",
    "                print_msg('unique clusters found in linear acceleration y: '+str(np.unique(dbscan.clusters[1]))+' total clusters: '+str(len(np.unique(dbscan.clusters[1]))))\n",
    "                print_msg('unique clusters found in linear acceleration y has the following cluster density: ')\n",
    "                for cluster in np.unique(dbscan.clusters[1]):\n",
    "                    print_msg('cluster: '+str(cluster)+' density: '+str(len(dbscan.clusters[1][dbscan.clusters[1] == cluster])))\n",
    "                print_msg('unique clusters found in linear acceleration z: '+str(np.unique(dbscan.clusters[2]))+' total clusters: '+str(len(np.unique(dbscan.clusters[2]))))\n",
    "                print_msg('unique clusters found in linear acceleration z has the follwing cluster density: ')\n",
    "                for cluster in np.unique(dbscan.clusters[2]):\n",
    "                    print_msg('cluster: '+str(cluster)+' density: '+str(len(dbscan.clusters[2][dbscan.clusters[2] == cluster])))\n",
    "                print_msg('unique clusters found in absolute acceleration: '+str(np.unique(dbscan.clusters[3]))+' total clusters: '+str(len(np.unique(dbscan.clusters[3]))))\n",
    "                print_msg('unique clusters found in absolute acceleration has the follwing cluster density: ')\n",
    "                for cluster in np.unique(dbscan.clusters[3]):\n",
    "                    print_msg('cluster: '+str(cluster)+' density: '+str(len(dbscan.clusters[3][dbscan.clusters[3] == cluster])))\n",
    "                # visualize a particular key in [gesture].data using DBSCAN clustering algorithm with different values of eps and min_samples\n",
    "                if key == False: # set to Key to visualize the data for each key in [gesture].data and False if you want to skip visualization for faster processing.\n",
    "                    dbscan.visualize_with_DataVisualization_2d(key+' '+str(e)+' '+str(m), include_original_data=True, sns=True, save=True, file_path_name=gesture.name+'/'+key+'_eps_'+str(e)+'_min_samples_'+str(m)+'.png', visualize_in_terminal=True)\n",
    "\n",
    "                best_values.append({\n",
    "                    'key': key, 'eps': e, 'min_samples': m,\n",
    "                    'unique_clusters_linear_acceleration_x': len(np.unique(dbscan.clusters[0])),\n",
    "                    'unique_clusters_linear_acceleration_y': len(np.unique(dbscan.clusters[1])),\n",
    "                    'unique_clusters_linear_acceleration_z': len(np.unique(dbscan.clusters[2])),\n",
    "                    'unique_clusters_absolute_acceleration': len(np.unique(dbscan.clusters[3])),\n",
    "                    'cluster_density_linear_acceleration_x': [len(dbscan.clusters[0][dbscan.clusters[0] == cluster]) for cluster in np.unique(dbscan.clusters[0])],\n",
    "                    'cluster_density_linear_acceleration_y': [len(dbscan.clusters[1][dbscan.clusters[1] == cluster]) for cluster in np.unique(dbscan.clusters[1])],\n",
    "                    'cluster_density_linear_acceleration_z': [len(dbscan.clusters[2][dbscan.clusters[2] == cluster]) for cluster in np.unique(dbscan.clusters[2])],\n",
    "                    'cluster_density_absolute_acceleration': [len(dbscan.clusters[3][dbscan.clusters[3] == cluster]) for cluster in np.unique(dbscan.clusters[3])]\n",
    "                })\n",
    "                \n",
    "    gesture.best_values_df = pd.DataFrame(best_values)\n",
    "    print_msg('gesture.best_value_df: '+str(gesture.best_values_df))\n",
    "    \n",
    "    # print gesture.best_values_df to a xlsx file\n",
    "    gesture.best_values_df.to_excel('best_values_df.xlsx', index=False)\n",
    "    gesture.tested_eps = eps\n",
    "    gesture.tested_min_samples = min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for gesture in [data_circles, data_come_here, data_go_away, data_wave]:\n",
    "# for gesture in [data_wave]:\n",
    "    keys = gesture.best_values_df['key'].unique()\n",
    "\n",
    "    # -----------------Data Visualization of eps and min_samples-----------------\n",
    "    if False: # set to True to visualize the data\n",
    "        # create a heatmap with index as eps and columns as min_samples and values as the number of unique clusters found in linear acceleration x, linear acceleration y, linear acceleration z, and absolute acceleration for each key in best_values_df.\n",
    "        dv = DataVisualization(gesture.best_values_df['eps'])\n",
    "        \n",
    "        for key in keys:\n",
    "            # create a heatmap for each key in gesture.best_values_df with index as eps and columns as min_samples and values as the number of unique clusters found in linear acceleration x, linear acceleration y, linear acceleration z, and absolute acceleration. make 4 seperate heatmaps for each acceleration in a grid of 2 x 2. use sns.heatmap to create the heatmap\n",
    "            dv.switch_to_seaborn(True)\n",
    "            dv.set_grid_params(2, 2, 20, 15, key)\n",
    "            sns.heatmap(gesture.best_values_df[gesture.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_linear_acceleration_x'), ax=dv.ax[0, 0], cmap='coolwarm')\n",
    "            dv.ax[0, 0].set_title('unique_clusters_linear_acceleration_x')\n",
    "            sns.heatmap(gesture.best_values_df[gesture.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_linear_acceleration_y'), ax=dv.ax[0, 1], cmap='coolwarm')\n",
    "            dv.ax[0, 1].set_title('unique_clusters_linear_acceleration_y')\n",
    "            sns.heatmap(gesture.best_values_df[gesture.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_linear_acceleration_z'), ax=dv.ax[1, 0], cmap='coolwarm')\n",
    "            dv.ax[1, 0].set_title('unique_clusters_linear_acceleration_z')\n",
    "            sns.heatmap(gesture.best_values_df[gesture.best_values_df['key'] == key].pivot(index='eps', columns='min_samples', values='unique_clusters_absolute_acceleration'), ax=dv.ax[1, 1], cmap='coolwarm')\n",
    "            dv.ax[1, 1].set_title('unique_clusters_absolute_acceleration')\n",
    "            dv.show_plot()\n",
    "            dv.switch_to_seaborn(False)\n",
    "    # -----------------Data Visualization of eps and min_samples-----------------\n",
    "\n",
    "    # -----------------Selecting the best values of eps and min_samples-----------------\n",
    "    # select the best values of eps and min_samples based on the number of unique clusters found in linear acceleration x, linear acceleration y, linear acceleration z, and absolute acceleration for each key in gesture.best_values_df.\n",
    "\n",
    "    gesture.aimed_unique_clusters = 3\n",
    "    \n",
    "    # -----------------Selecting the best values of eps and min_samples----------------\n",
    "\n",
    "    # testing calculating the best values of eps and min_samples together.\n",
    "    min_samples_and_eps_mean_unique_clusters = []\n",
    "    for e in eps:\n",
    "        for m in min_samples:\n",
    "            mean_unique_clusters = []\n",
    "            for key in keys:\n",
    "                mean_unique_clusters.append({\n",
    "                    'key': key,\n",
    "                    'mean_unique_clusters_linear_acceleration_x': gesture.best_values_df[(gesture.best_values_df['key'] == key) & (gesture.best_values_df['eps'] == e) & (gesture.best_values_df['min_samples'] == m)]['unique_clusters_linear_acceleration_x'].mean(),\n",
    "                    'mean_unique_clusters_linear_acceleration_y': gesture.best_values_df[(gesture.best_values_df['key'] == key) & (gesture.best_values_df['eps'] == e) & (gesture.best_values_df['min_samples'] == m)]['unique_clusters_linear_acceleration_y'].mean(),\n",
    "                    'mean_unique_clusters_linear_acceleration_z': gesture.best_values_df[(gesture.best_values_df['key'] == key) & (gesture.best_values_df['eps'] == e) & (gesture.best_values_df['min_samples'] == m)]['unique_clusters_linear_acceleration_z'].mean(),\n",
    "                    'mean_unique_clusters_absolute_acceleration': gesture.best_values_df[(gesture.best_values_df['key'] == key) & (gesture.best_values_df['eps'] == e) & (gesture.best_values_df['min_samples'] == m)]['unique_clusters_absolute_acceleration'].mean()\n",
    "                })\n",
    "            # mean is calculated a second time to get the mean of the mean of the unique clusters for each key.\n",
    "            min_samples_and_eps_mean_unique_clusters.append({\n",
    "                'eps': e,\n",
    "                'min_samples': m,\n",
    "                'mean_unique_clusters_linear_acceleration_x': pd.DataFrame(mean_unique_clusters)['mean_unique_clusters_linear_acceleration_x'].mean(),\n",
    "                'mean_unique_clusters_linear_acceleration_y': pd.DataFrame(mean_unique_clusters)['mean_unique_clusters_linear_acceleration_y'].mean(),\n",
    "                'mean_unique_clusters_linear_acceleration_z': pd.DataFrame(mean_unique_clusters)['mean_unique_clusters_linear_acceleration_z'].mean(),\n",
    "                'mean_unique_clusters_absolute_acceleration': pd.DataFrame(mean_unique_clusters)['mean_unique_clusters_absolute_acceleration'].mean()\n",
    "            })\n",
    "            \n",
    "    gesture.min_samples_and_eps_mean_unique_clusters_df = pd.DataFrame(min_samples_and_eps_mean_unique_clusters)\n",
    "    # print_msg('gesture.min_samples_and_eps_mean_unique_clusters_df: '+str(gesture.min_samples_and_eps_mean_unique_clusters_df))\n",
    "\n",
    "    # select the best eps and min_samples for linear acceleration x\n",
    "    gesture.best_eps_linear_acceleration_x = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_linear_acceleration_x'] - gesture.aimed_unique_clusters).abs().argsort()[:1]]['eps'].values[0]\n",
    "    gesture.best_min_samples_linear_acceleration_x = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_linear_acceleration_x'] - gesture.aimed_unique_clusters).abs().argsort()[:1]]['min_samples'].values[0]\n",
    "    print_msg(gesture.name+' -> best_eps_linear_acceleration_x.min_samples_and_eps_mean_unique_clusters_df: '+str(gesture.best_eps_linear_acceleration_x))\n",
    "    print_msg(gesture.name+' -> best_min_samples_linear_acceleration_x: '+str(gesture.best_min_samples_linear_acceleration_x))\n",
    "\n",
    "    # select the best eps and min_samples for linear acceleration y\n",
    "    gesture.best_eps_linear_acceleration_y = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_linear_acceleration_y'] - gesture.aimed_unique_clusters).abs().argsort()[:1]]['eps'].values[0]\n",
    "    gesture.best_min_samples_linear_acceleration_y = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_linear_acceleration_y'] - gesture.aimed_unique_clusters).abs().argsort()[:1]]['min_samples'].values[0]\n",
    "    print_msg(gesture.name+' -> best_eps_linear_acceleration_y: '+str(gesture.best_eps_linear_acceleration_y))\n",
    "    print_msg(gesture.name+' -> best_min_samples_linear_acceleration_y: '+str(gesture.best_min_samples_linear_acceleration_y))\n",
    "\n",
    "    # select the best eps and min_samples for linear acceleration z\n",
    "    gesture.best_eps_linear_acceleration_z = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_linear_acceleration_z'] - gesture.aimed_unique_clusters).abs().argsort()[:1]]['eps'].values[0]\n",
    "    gesture.best_min_samples_linear_acceleration_z = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_linear_acceleration_z'] - gesture.aimed_unique_clusters).abs().argsort()[:1]]['min_samples'].values[0]\n",
    "    print_msg(gesture.name+' -> best_eps_linear_acceleration_z: '+str(gesture.best_eps_linear_acceleration_z))\n",
    "    print_msg(gesture.name+' -> best_min_samples_linear_acceleration_z: '+str(gesture.best_min_samples_linear_acceleration_z))\n",
    "\n",
    "    # select the best eps and min_samples for absolute acceleration\n",
    "    gesture.best_eps_absolute_acceleration = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_absolute_acceleration'] - gesture.aimed_unique_clusters).abs().argsort()[:1]]['eps'].values[0]\n",
    "    gesture.best_min_samples_absolute_acceleration = gesture.min_samples_and_eps_mean_unique_clusters_df.iloc[(gesture.min_samples_and_eps_mean_unique_clusters_df['mean_unique_clusters_absolute_acceleration'] - gesture.aimed_unique_clusters).abs().argsort()[:1]]['min_samples'].values[0]\n",
    "    print_msg(gesture.name+' -> best_eps_absolute_acceleration: '+str(gesture.best_eps_absolute_acceleration))\n",
    "    print_msg(gesture.name+' -> best_min_samples_absolute_acceleration: '+str(gesture.best_min_samples_absolute_acceleration))\n",
    "\n",
    "\n",
    "    # -----------------Selecting the best values of eps and min_samples----------------\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# -----------------labelling the data using the best values of eps and min_samples----------------\n",
    "# label the data using the best values of eps and min_samples for each key in data_wave.data\n",
    "for gesture in [data_circles, data_come_here, data_go_away, data_wave]:\n",
    "# for gesture in [data_wave]:\n",
    "    keys = gesture.data.keys()\n",
    "    for key in keys:\n",
    "        print_sys_msg(key)\n",
    "        print_msg('columns being worked on are: '+str(gesture.data[key].columns))\n",
    "\n",
    "        print_msg('best_eps_linear_acceleration_x: '+str(gesture.best_eps_linear_acceleration_x))\n",
    "        print_msg('best_min_samples_linear_acceleration_x: '+str(gesture.best_min_samples_linear_acceleration_x))\n",
    "        dbscan = DBSCANClustering(gesture.data[key], gesture.best_eps_linear_acceleration_x, gesture.best_min_samples_linear_acceleration_x)\n",
    "        dbscan.cluster()\n",
    "        gesture.data[key]['DBSCAN Clustering linear acceleration x'] = dbscan.clusters[0]\n",
    "\n",
    "        print_msg('best_eps_linear_acceleration_y: '+str(gesture.best_eps_linear_acceleration_y))\n",
    "        print_msg('best_min_samples_linear_acceleration_y: '+str(gesture.best_min_samples_linear_acceleration_y))\n",
    "        dbscan = DBSCANClustering(gesture.data[key], gesture.best_eps_linear_acceleration_y, gesture.best_min_samples_linear_acceleration_y)\n",
    "        dbscan.cluster()\n",
    "        gesture.data[key]['DBSCAN Clustering linear acceleration y'] = dbscan.clusters[1]\n",
    "\n",
    "        print_msg('best_eps_linear_acceleration_z: '+str(gesture.best_eps_linear_acceleration_z))\n",
    "        print_msg('best_min_samples_linear_acceleration_z: '+str(gesture.best_min_samples_linear_acceleration_z))\n",
    "        dbscan = DBSCANClustering(gesture.data[key], gesture.best_eps_linear_acceleration_z, gesture.best_min_samples_linear_acceleration_z)\n",
    "        dbscan.cluster()\n",
    "        gesture.data[key]['DBSCAN Clustering linear acceleration z'] = dbscan.clusters[2]\n",
    "\n",
    "        print_msg('best_eps_absolute_acceleration: '+str(gesture.best_eps_absolute_acceleration))\n",
    "        print_msg('best_min_samples_absolute_acceleration: '+str(gesture.best_min_samples_absolute_acceleration))\n",
    "        dbscan = DBSCANClustering(gesture.data[key], gesture.best_eps_absolute_acceleration, gesture.best_min_samples_absolute_acceleration)\n",
    "        dbscan.cluster()\n",
    "        gesture.data[key]['DBSCAN Clustering absolute acceleration'] = dbscan.clusters[3]\n",
    "        print_msg('columns after labelling are: '+str(gesture.data[key].columns))\n",
    "\n",
    "        # combine the appropriate cluster labels for each key in gesture.data\n",
    "        gesture.clusters = [gesture.data[key]['DBSCAN Clustering linear acceleration x'], gesture.data[key]['DBSCAN Clustering linear acceleration y'], gesture.data[key]['DBSCAN Clustering linear acceleration z'], gesture.data[key]['DBSCAN Clustering absolute acceleration']]\n",
    "        \n",
    "        if False: # set to True to visualize the labelled data\n",
    "            dv = DataVisualization(gesture.data[key]['Time (s)'])\n",
    "            dv.switch_to_seaborn(True)\n",
    "\n",
    "            dv.set_grid_params(2, 2, 20, 10, gesture.name+' -> '+key+' -> DBSCAN Clustering')\n",
    "\n",
    "            dv.add_data(gesture.data[key].iloc[:, 1])\n",
    "            dv.plot_grid_2d(0, 0, 'scatter', 'Time (s)', gesture.data[key].columns[1], gesture.data[key].columns[1] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_x)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_x), c=gesture.clusters[0])\n",
    "            dv.plot_grid_2d(0, 0, 'line' , 'Time (s)', gesture.data[key].columns[1], gesture.data[key].columns[1] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_x)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_x), c='red', thickness=0.3)\n",
    "\n",
    "            dv.add_data(gesture.data[key].iloc[:, 2])\n",
    "            dv.plot_grid_2d(0, 1, 'scatter', 'Time (s)', gesture.data[key].columns[2], gesture.data[key].columns[2] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_y)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_y), c=gesture.clusters[1])\n",
    "            dv.plot_grid_2d(0, 1, 'line' , 'Time (s)', gesture.data[key].columns[2], gesture.data[key].columns[2] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_y)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_y), c='red', thickness=0.3)\n",
    "\n",
    "            dv.add_data(gesture.data[key].iloc[:, 3])\n",
    "            dv.plot_grid_2d(1, 0, 'scatter', 'Time (s)', gesture.data[key].columns[3], gesture.data[key].columns[3] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_z)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_z), c=gesture.clusters[2])\n",
    "            dv.plot_grid_2d(1, 0, 'line' , 'Time (s)', gesture.data[key].columns[3], gesture.data[key].columns[3] + ' -> eps: '+str(gesture.best_eps_linear_acceleration_z)+' min_samples: '+str(gesture.best_min_samples_linear_acceleration_z), c='red', thickness=0.3)\n",
    "\n",
    "            dv.add_data(gesture.data[key].iloc[:, 4])\n",
    "            dv.plot_grid_2d(1, 1, 'scatter', 'Time (s)', gesture.data[key].columns[4], gesture.data[key].columns[4] + ' -> eps: '+str(gesture.best_eps_absolute_acceleration)+' min_samples: '+str(gesture.best_min_samples_absolute_acceleration), c=gesture.clusters[3])\n",
    "            dv.plot_grid_2d(1, 1, 'line' , 'Time (s)', gesture.data[key].columns[4], gesture.data[key].columns[4] + ' -> eps: '+str(gesture.best_eps_absolute_acceleration)+' min_samples: '+str(gesture.best_min_samples_absolute_acceleration), c='red', thickness=0.3)\n",
    "\n",
    "            dv.show_plot()\n",
    "\n",
    "            dv.switch_to_seaborn(False)\n",
    "\n",
    "# -----------------Deciding sliding window size----------------\n",
    "# using clusters to decide the sliding window size uniform across all keys and all accelerations\n",
    "# the sliding window size is decided based on the number of unique clusters found in the data\n",
    "\n",
    "# marking the vertical regions where the number of clusters are denser on the timeline to decide the sliding window size\n",
    "\n",
    "gesture.avg_sliding_window_size = []\n",
    "gesture.avg_sliding_window_step_size = []\n",
    "\n",
    "for gesture in [data_circles, data_come_here, data_go_away, data_wave]:\n",
    "    keys = gesture.data.keys()\n",
    "    for key in keys:\n",
    "        print_sys_msg(key)\n",
    "        print_msg('columns being worked on are: '+str(gesture.data[key].columns))\n",
    "        \n",
    "        # selecting the clusters to focus on for the sliding window size. After a particular cluster is selected, we can follow it's density on the timeline to decide the sliding window size.\n",
    "        selected_cluster = 0\n",
    "        gesture.selected_cluster = selected_cluster\n",
    "        gesture.selected_cluster_density = gesture.data[key][gesture.data[key]['DBSCAN Clustering linear acceleration x'] == selected_cluster]['DBSCAN Clustering linear acceleration x']\n",
    "        gesture.selected_cluster_density = gesture.selected_cluster_density.append(gesture.data[key][gesture.data[key]['DBSCAN Clustering linear acceleration y'] == selected_cluster]['DBSCAN Clustering linear acceleration y'])\n",
    "        gesture.selected_cluster_density = gesture.selected_cluster_density.append(gesture.data[key][gesture.data[key]['DBSCAN Clustering linear acceleration z'] == selected_cluster]['DBSCAN Clustering linear acceleration z'])\n",
    "        gesture.selected_cluster_density = gesture.selected_cluster_density.append(gesture.data[key][gesture.data[key]['DBSCAN Clustering absolute acceleration'] == selected_cluster]['DBSCAN Clustering absolute acceleration'])\n",
    "        gesture.selected_cluster_density = gesture.selected_cluster_density.sort_values()\n",
    "        gesture.selected_cluster_density = gesture.selected_cluster_density.reset_index(drop=True)\n",
    "        print_msg('gesture.selected_cluster_density: '+str(gesture.selected_cluster_density))\n",
    "        \n",
    "        # calculating the sliding window size and step size based on the density of the selected cluster\n",
    "        gesture.sliding_window_size = 0\n",
    "        gesture.sliding_window_step_size = 0\n",
    "        gesture.sliding_window_size_list = []\n",
    "        gesture.sliding_window_step_size_list = []\n",
    "        for i in range(1, len(gesture.selected_cluster_density)):\n",
    "            gesture.sliding_window_size = gesture.selected_cluster_density[i] - gesture.selected_cluster_density[i-1]\n",
    "            gesture.sliding_window_size_list.append(gesture.sliding_window_size)\n",
    "            gesture.sliding_window_step_size = gesture.sliding_window_size\n",
    "            gesture.sliding_window_step_size_list.append(gesture.sliding_window_step_size)\n",
    "        gesture.avg_sliding_window_size.append(np.mean(gesture.sliding_window_size_list))\n",
    "        gesture.avg_sliding_window_step_size.append(np.mean(gesture.sliding_window_step_size_list))\n",
    "        print_msg('gesture.sliding_window_size: '+str(gesture.sliding_window_size))\n",
    "        print_msg('gesture.sliding_window_step_size: '+str(gesture.sliding_window_step_size))\n",
    "        print_msg('gesture.avg_sliding_window_size: '+str(gesture.avg_sliding_window_size))\n",
    "        print_msg('gesture.avg_sliding_window_step_size: '+str(gesture.avg_sliding_window_step_size))\n",
    "\n",
    "        # visualizing the selected cluster density on the timeline\n",
    "        dv = DataVisualization(gesture.data[key]['Time (s)'])\n",
    "        dv.switch_to_seaborn(True)\n",
    "        dv.add_data(gesture.data[key].iloc[:, 1])\n",
    "        dv.plot_chart('line', 'red', 0.3)\n",
    "        dv.add_data(gesture.data[key].iloc[:, 2])\n",
    "        dv.plot_chart('line', 'green', 0.3)\n",
    "        dv.add_data(gesture.data[key].iloc[:, 3])\n",
    "        dv.plot_chart('line', 'blue', 0.3)\n",
    "        dv.add_data(gesture.data[key].iloc[:, 4])\n",
    "        dv.plot_chart('line', 'black', 0.3)\n",
    "        dv.add_data(gesture.selected_cluster_density)\n",
    "        dv.plot_chart('scatter', 'cyan', 0.8)\n",
    "        dv.set_x_label('Time (s)')\n",
    "        dv.set_y_label('Selected Cluster Density')\n",
    "        dv.set_title('Selected Cluster Density')\n",
    "        dv.show_plot()\n",
    "        dv.switch_to_seaborn(False)\n",
    "\n",
    "        break\n",
    "\n",
    "        \n",
    "# -----------------Deciding sliding window size----------------\n",
    "# -----------------labelling the data using the best values of eps and min_samples----------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a visualization of moving sliding window over dataframe of each key in data.data.keys\n",
    "# for gesture in [data_circles, data_come_here, data_go_away, data_wave]:\n",
    "for gesture in [data_wave]:\n",
    "    gesture.stw = SlidingTimeWindow(data_wave.data[key], 100, 10)\n",
    "    for key in gesture.data.keys():\n",
    "        print_sys_msg(key)\n",
    "        \n",
    "        gesture.stw.create_frames()\n",
    "        frames = gesture.stw.get_frames()\n",
    "        print_sys_msg('Number of frames: '+str(len(frames)))\n",
    "        # continue\n",
    "        for frame in frames:\n",
    "            dv = DataVisualization(frame['Time (s)'])\n",
    "            dv.add_data(frame['Linear Acceleration y (m/s^2)'])\n",
    "            # dv.set_grid_params(1, 1, 20, 5, 'Linear Acceleration x (m/s^2) vs Time (s)')\n",
    "            # dv.plot_grid_1d(1, 'line', 'Time (s)', 'Linear Acceleration x (m/s^2)', 'Linear Acceleration x (m/s^2) vs Time (s)', \"red\")\n",
    "            dv.plot_chart('line', 'Time (s)', 'Linear Acceleration x (m/s^2)', 'Linear Acceleration x (m/s^2) vs Time (s)', \"red\")\n",
    "            dv.show_plot()\n",
    "        break\n",
    "\n",
    "\n",
    "    # topics to cover to run the code are - DBSCAN, Sliding Time Window, Clustering analysis, Normalization,  standardization, test-train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data_wave use cluster analysis to divide the data in each data key into two clusters based on the distance between the points in the data. Essentially, we are trying to distinguish between low change in accelaration and high change in accelaration to identify and lable the sections as gestures and non-gesture sections.\n",
    "\n",
    "for key in data_wave.data.keys():\n",
    "    print_sys_msg(key)\n",
    "    # print type of data_wave.data[key]\n",
    "    print_sys_msg(data_wave.data[key].dtypes)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
